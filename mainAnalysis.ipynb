{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pingouin as pg\n",
    "import pymer4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Definitions and Constants\n",
    "\n",
    "Here I declare all necessary variables and constants. This includes the snippet names as well as the correct answers for each snippet. I further add the possible variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_answer: dict[str, str] = {\n",
    "\t'arrayAverage': '4.0', \n",
    "\t'binarySearch': '1', \n",
    "\t'binaryToDecimal': '13', \n",
    "\t'bubbleSort': '[1,2,3,4,5]', \n",
    "\t'capitalizeFirstLetter': 'Hello World', \n",
    "\t'commonChars': '2', \n",
    "\t'containsSubstring': 'True', \n",
    "\t'countIntegerInterval': '4', \n",
    "\t'countLetters': '4',\n",
    "\t'crossSum': '16',\n",
    "\t'factorial': '24', \n",
    "\t'forwardBackward': 'pricelesssselecirp', \n",
    "\t'leastCommonMultiple': '30',\n",
    "\t'linearSearch': '1',\n",
    "\t'palindrome': 'True', \n",
    "\t'power': '8',\n",
    "\t'prime': 'True', \n",
    "\t'squareRoot': '[3.0, 5.0, 4.0, 10.0]', \n",
    "\t'unrolledSort': '[8, 9, 11, 12]', \n",
    "\t'validParentheses': 'False',\n",
    "\t'WarmUp': 'oefl',\n",
    "}\n",
    "\n",
    "possible_variations: dict[str, list[str]] = {\n",
    "\t'group_meaningful': ['MT', 'MN'],\n",
    "\t'group_meaningless': ['LT', 'LN'],\n",
    "}\n",
    "\n",
    "# The following lists are used to create a dataframe for the General Information\n",
    "studentQuestions: list[str] = [\n",
    "\t'StudyBefore',\n",
    "\t'Job',\n",
    "\t'CourseOfStudy',\n",
    "\t'Semester',\n",
    "\t'Algorithms',\n",
    "\t'NrFalseInputs_studentQuestions',\n",
    "]\n",
    "progQuestions: list[str] = [\n",
    "\t'YearsProgramming',\n",
    "\t'ProgrammingLately',\n",
    "\t'ProgrammingLanguages',\n",
    "\t'RecentProgrammingLanguages',\n",
    "\t'PythonProgramming',\n",
    "\t'OverallExperience',\n",
    "\t'Classmates',\n",
    "\t'NrFalseInputs_progQuestions',\n",
    "]\n",
    "generalQuestions: list[str] = [\n",
    "\t'Age',\n",
    "\t'Gender',\n",
    "\t'Eyesight',\n",
    "\t'NrFalseInputs_generalQuestions',\n",
    "]\n",
    "miscellaneous_general_information: list[str] = [\n",
    "\t'ActualScreenWidth',\n",
    "\t'ActualScreenHeight',\n",
    "\t'EyeXScreenWidth',\n",
    "\t'EyeXScreenHeight',\n",
    "\t'SubjectID',\n",
    "]\n",
    "\n",
    "# The following lists are used to create a dataframe for the Correctness and Time Data\n",
    "resultsOverall: list[str] = [\n",
    "\t'Number',\n",
    "\t'Task',\n",
    "\t'Answer_Out',\n",
    "\t'Time',\n",
    "\t'TimeOut',\n",
    "\t'SubjectID',\n",
    "\t'CorrectAnswer',\n",
    "\t'Meaningful',\n",
    "\t'TypeAnnotation',\n",
    "]\n",
    "\n",
    "necessary_columns: list[str] = [\n",
    "\t'ID',\n",
    "]\n",
    "\n",
    "meta_data_columns: list[str] = [\n",
    "    'GazeData', \n",
    "    'GazeDataFilled', \n",
    "    'PersonalInformation', \n",
    "    'ResultsOverall', \n",
    "    'PostQuestionnaire', \n",
    "    'DifficultyRating', \n",
    "    'InterviewData', \n",
    "    'OverallTime', \n",
    "    'Meaningful', \n",
    "    'Finished', \n",
    "    'NumberOfMissingSnippets', \n",
    "    'TrialData',\n",
    "]\n",
    "\n",
    "likert_mapping: dict[str, int] = {\n",
    "    'Very Inexperienced': 0,\n",
    "    'Inexperienced': 1,\n",
    "    'Neutral': 2,\n",
    "    'Experienced': 3,\n",
    "    'Very Experienced': 4,\n",
    "}\n",
    "\n",
    "figure_path = '../Thesis/gfx'\n",
    "\n",
    "difficulty_type = CategoricalDtype(categories=[1, 2, 3, 4, 5], ordered=True)\n",
    "experience_type = CategoricalDtype(categories=['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'], ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants: list[str] = [] # list of participant numbers\n",
    "meaningful_participants: list[str] = [] # list of participant numbers who were assigned to the meaningful group\n",
    "meaningless_participants: list[str] = [] # list of participant numbers who were assigned to the meaningless group\n",
    "\n",
    "df_personal_information: DataFrame = pd.DataFrame(columns=necessary_columns + generalQuestions + studentQuestions + progQuestions + miscellaneous_general_information)\n",
    "df_meta_data: DataFrame = pd.DataFrame(columns=necessary_columns + meta_data_columns)\n",
    "df_difficulty_rating: DataFrame = pd.DataFrame(columns=['ID', 'Task', 'Difficulty', 'Comment'])\n",
    "df_subjective_feelings: DataFrame = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally helpful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter a plot and give it a new color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_box_properties(plot_name, color_code, label, axis = None):\n",
    "    for k, v in plot_name.items():\n",
    "        plt.setp(plot_name.get(k), color=color_code)\n",
    "         \n",
    "    # use plot function to draw a small line to name the legend\n",
    "    if axis is not None:\n",
    "        axis.plot([], c=color_code, label=label)\n",
    "        axis.legend()\n",
    "    else:\n",
    "        plt.plot([], c=color_code, label=label)\n",
    "        plt.legend()\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = round(stats.pearsonr(tmp[r], tmp[c])[1], 4)\n",
    "    return pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all data\n",
    "\n",
    "This includes:\n",
    "- [x] ~~Gaze Data~~\n",
    "- [x] ~~Individual Summary~~\n",
    "- [x] ~~Personal Information~~\n",
    "- [x] ~~Correctness and Time~~\n",
    "- [x] ~~Interview Data~~\n",
    "\n",
    "First we need to find all possible folders and files. They are located in `Participants`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants: list[str]  = sorted(os.walk('./AllParticipants').__next__()[1])\n",
    "df_meta_data['ID'] = participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for all files\n",
    "\n",
    "Collect all files that are present and collect which are still missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  please check if file '`ID` PostQuestionaire.pdf' exists\n",
    "for participant in participants:\n",
    "    # check if '`ID` Post-Questionnaire.pdf' exists\n",
    "    if not os.path.isfile(f'./AllParticipants/{participant}/{participant} Post-Questionnaire.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PostQuestionnaire'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PostQuestionnaire'] = True\n",
    "    \n",
    "    # check if '`ID`' exists\n",
    "    if not os.path.isfile(f'./AllParticipants/{participant}/DifficultyRating_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'DifficultyRating'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'DifficultyRating'] = True\n",
    "    \n",
    "    # check if the folder 'Trial_`ID`' exists\n",
    "    if not os.path.isdir(f'./AllParticipants/{participant}/Trial_{participant}'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'TrialData'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'TrialData'] = True\n",
    "    \n",
    "    # check if 'GeneralInfo_`ID`.csv' exists\n",
    "    if not os.path.isfile(f'./AllParticipants/{participant}/Trial_{participant}/GeneralInfo_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PersonalInformation'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PersonalInformation'] = True\n",
    "    \n",
    "    # check if 'ResultsOverall_`ID`.csv' exists\n",
    "    if not os.path.isfile(f'./AllParticipants/{participant}/Trial_{participant}/ResultsOverall_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'ResultsOverall'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'ResultsOverall'] = True\n",
    "    \n",
    "    # check if 'GazeData_`ID`.csv' exists\n",
    "    if not os.path.isfile(f'./AllParticipants/{participant}/Trial_{participant}/GazeData_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'GazeData'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'GazeData'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all Meta Information about Files concerning the study itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the count of False GazeData vs the count of True GazeData and show which ones are currently missing\n",
    "gaze_data_count = df_meta_data['GazeData'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('GazeDataFiles found:', gaze_data_count[True], len(df_meta_data['GazeData'])))\n",
    "\n",
    "# print the count of False PersonalInformation vs the count of True PersonalInformation and show which ones are currently missing\n",
    "personal_information_count = df_meta_data['PersonalInformation'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('PersonalInformationFiles found:', personal_information_count[True], len(df_meta_data['PersonalInformation'])))\n",
    "\n",
    "# print the count of False ResultsOverall vs the count of True ResultsOverall and show which ones are currently missing\n",
    "results_overall_count = df_meta_data['ResultsOverall'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('ResultsOverallFiles found:', results_overall_count[True], len(df_meta_data['ResultsOverall'])))\n",
    "\n",
    "# print the count of False PostQuestionnaire vs the count of True PostQuestionnaire and show which ones are currently missing\n",
    "post_questionnaire_count = df_meta_data['PostQuestionnaire'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('PostQuestionnaireFiles found:', post_questionnaire_count[True], len(df_meta_data['PostQuestionnaire'])))\n",
    "\n",
    "# print the count of False DifficultyRating vs the count of True DifficultyRating and show which ones are currently missing\n",
    "difficulty_rating_count = df_meta_data['DifficultyRating'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('DifficultyRatingFiles found:', difficulty_rating_count[True], len(df_meta_data['DifficultyRating'])))\n",
    "\n",
    "# print the count of False TrialData vs the count of True TrialData and show which ones are currently missing\n",
    "trial_data_count = df_meta_data['TrialData'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('TrialDataFolders found:', trial_data_count[True], len(df_meta_data['TrialData'])))\n",
    "\n",
    "# print which ones are missing\n",
    "print('{:56} {}'.format('The following participants have no GazeData:', list(df_meta_data.loc[df_meta_data['GazeData'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no PersonalInformation:', list(df_meta_data.loc[df_meta_data['PersonalInformation'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no ResultsOverall:', list(df_meta_data.loc[df_meta_data['ResultsOverall'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no PostQuestionnaire:', list(df_meta_data.loc[df_meta_data['PostQuestionnaire'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no DifficultyRating:', list(df_meta_data.loc[df_meta_data['DifficultyRating'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no TrialData:', list(df_meta_data.loc[df_meta_data['TrialData'] == False, 'ID'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the `df_meta_data` DataFrame of columns that are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_columns_meta_data: list[str] = [\n",
    "    'GazeData', \n",
    "    'PersonalInformation', \n",
    "    'ResultsOverall', \n",
    "    'PostQuestionnaire', \n",
    "    'DifficultyRating', \n",
    "    'TrialData',\n",
    "]\n",
    "\n",
    "# remove unnecessary columns from df_meta_data\n",
    "try:\n",
    "    df_meta_data.drop(columns=unnecessary_columns_meta_data, inplace=True)\n",
    "\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Information\n",
    "\n",
    "I read in all data from the files containing the personal information (`GeneralInfo_{ID}`) and add the `ID` for the participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(participants)):\n",
    "\ttry:\n",
    "\t\tdf_personal_information.loc[i] = pd.read_csv(f'./AllParticipants/{participants[i]}/Trial_{participants[i]}/GeneralInfo_{participants[i]}.csv', sep=';').iloc[0]\n",
    "\t\tdf_personal_information.loc[i, 'ID'] = participants[i]\n",
    "\t\tdf_meta_data.loc[i, 'PersonalInformation'] = True\n",
    "\t\t\n",
    "\texcept:\n",
    "\t\tprint(f'Participant {participants[i]} has no GeneralInfo.csv file')\n",
    "\t\tdf_meta_data.loc[i, 'PersonalInformation'] = False\n",
    "\n",
    "# write personal_information into a csv\n",
    "df_personal_information.to_csv(f'./eyetracking/studies/Linearity/GeneralInfo_AllParticipants.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty Rating\n",
    "\n",
    "Read in all the difficulty rating to merge it later on into the results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(participants)):\n",
    "\ttry:\n",
    "\t\tdf_to_add = pd.read_csv(f'./AllParticipants/{participants[i]}/DifficultyRating_{participants[i]}.csv', sep=';')\n",
    "\t\tif not all([x in snippet_answer.keys() for x in df_to_add[\"Task\"]]):\n",
    "\t\t\tprint(f'Participant {participants[i]} has a problem in the DifficultyRating file; these names {[x for x in df_to_add[\"Task\"] if x not in snippet_answer.keys()]} are not in the snippet_answer dictionary')\n",
    "\n",
    "\t\tdf_to_add['ID'] = participants[i]\n",
    "\t\tdf_difficulty_rating = pd.concat([df_difficulty_rating, df_to_add], ignore_index=True)\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "df_difficulty_rating['Difficulty'] = df_difficulty_rating['Difficulty'].astype(difficulty_type)\n",
    "\n",
    "# df_difficulty_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjective Feelings\n",
    "\n",
    "Read in all subjective feelings to merge it later on into the personal information DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(participants)):\n",
    "    try:\n",
    "        df_to_add = pd.read_csv(f'./AllParticipants/{participants[i]}/{participants[i]} Post-Questionnaire.csv', sep=';')\n",
    "        df_to_add['ID'] = participants[i]\n",
    "        df_subjective_feelings = pd.concat([df_subjective_feelings, df_to_add], ignore_index=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning for Personal Information\n",
    "\n",
    "We can drop all columns that contain the number of false Inputs. We can further remove anyone who is currently not in their Bachelor's degree. These people shall be mentioned to see if they incorrectly filled out the form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_columns_gen_info: list[str] = [\n",
    "\t'NrFalseInputs_studentQuestions',\n",
    "\t'NrFalseInputs_progQuestions',\n",
    "\t'NrFalseInputs_generalQuestions',\n",
    "]\n",
    "\n",
    "# remove unnecessary columns\n",
    "try:\n",
    "\tdf_personal_information.drop(columns=unnecessary_columns_gen_info, inplace=True)\n",
    "\n",
    "except:\n",
    "\tpass\n",
    "\n",
    "# remove participants that are not currently in their bachelor but also mention them\n",
    "print(df_personal_information.query('Job != \"Undergraduate Student (Bachelor Studies)\"')['ID'])\n",
    "df_personal_information.drop(df_personal_information[df_personal_information['Job'] != 'Undergraduate Student (Bachelor Studies)'].index, inplace=True)\n",
    "\n",
    "df_personal_information['PythonProgramming'] = df_personal_information['PythonProgramming'].astype(experience_type)\n",
    "df_personal_information['OverallExperience'] = df_personal_information['OverallExperience'].astype(experience_type)\n",
    "df_personal_information['Classmates'] = df_personal_information['Classmates'].astype(experience_type)\n",
    "\n",
    "category_mapping = {'Very Inexperienced': 1, 'Inexperienced': 2, 'Average': 3, 'Experienced': 4, 'Very Experienced': 5}\n",
    "df_personal_information['Classmates'] = df_personal_information['Classmates'].map(category_mapping)\n",
    "df_personal_information['PythonProgramming'] = df_personal_information['PythonProgramming'].map(category_mapping)\n",
    "df_personal_information['OverallExperience'] = df_personal_information['OverallExperience'].map(category_mapping)\n",
    "\n",
    "df_personal_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjective Difficulty Merging\n",
    "\n",
    "Merging the subjective feelings into the personal information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge personal information and the subjective feelings\n",
    "df_personal_information = pd.merge(df_personal_information, df_subjective_feelings, how='left', left_on=['ID'], right_on=['ID'])\n",
    "\n",
    "# df_personal_information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of the personal information\n",
    "\n",
    "1. Correct and throw out the Bachelor students\n",
    "2. Correct and throw out any person that is not in CS-related courses of study\n",
    "3. Throw out anyone under the age of 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_of_study_mapping: dict[str, str] = {\n",
    "    'BCs. Informatik': 'Computer Science',\n",
    "    'Cybersicherheit': 'Cybersecurity',\n",
    "    'DSAI': 'Data Science and Artificial Intelligence',\n",
    "    'B. Sc. Business Informatics': 'Business Informatics',\n",
    "    'Informatik B.Sc': 'Computer Science',\n",
    "    'Eingebettete Systeme': 'Embedded Systems',\n",
    "    'Informatik': 'Computer Science',\n",
    "    'Math and Computer Science': 'Mathematics and Computer Science',\n",
    "    'Computer Science (German)': 'Computer Science',\n",
    "    'Mathematik und Informatik': 'Mathematics and Computer Science',\n",
    "    'Medieninformatik': 'Media Informatics',\n",
    "    'medieninfo': 'Media Informatics',\n",
    "    'Informatics': 'Computer Science',\n",
    "    'Informatik Kernbereich': 'Computer Science',\n",
    "    'Eingebettete Systeme B.Sc.': 'Embedded Systems',\n",
    "    'Computerlinguistik B.Sc.': 'Computational Linguistics',\n",
    "}\n",
    "\n",
    "possible_courses_of_study: list[str] = ['Computer Science', \n",
    "                                        'Cybersecurity', \n",
    "                                        'Business Informatics', \n",
    "                                        'Data Science and Artificial Intelligence',\n",
    "                                        'Mathematics and Computer Science',\n",
    "                                        'Media Informatics',\n",
    "                                        'Embedded Systems',\n",
    "                                        'Bioinformatics',\n",
    "                                        'Computational Linguistics',\n",
    "                                        ]\n",
    "\n",
    "# check the age and throw out anyone under the age of 18\n",
    "print(f'Have to drop {df_personal_information.query(\"Age < 18\")[\"ID\"].tolist()} because they were under the age of 18')\n",
    "df_personal_information.drop(df_personal_information[df_personal_information['Age'] < 18].index, inplace=True)\n",
    "\n",
    "# check and throw out anyone that is not a bachelors student\n",
    "non_bachelors: list[str] = df_personal_information.query(\"Job != 'Undergraduate Student (Bachelor Studies)'\")[\"ID\"].tolist()\n",
    "print(f'Have to drop {non_bachelors} because they were not in their Bachelors')\n",
    "df_personal_information.drop(df_personal_information[df_personal_information['Job'] != 'Undergraduate Student (Bachelor Studies)'].index, inplace=True)\n",
    "\n",
    "# normalize all the course of study values\n",
    "df_personal_information['CourseOfStudy'].replace(course_of_study_mapping, inplace=True)\n",
    "print(f'Have to drop {df_personal_information[~df_personal_information[\"CourseOfStudy\"].isin(possible_courses_of_study)][\"CourseOfStudy\"].tolist()} because they were not in the course of study mapping')\n",
    "df_personal_information.drop(df_personal_information[~df_personal_information['CourseOfStudy'].isin(possible_courses_of_study)].index, inplace=True)\n",
    "\n",
    "# check and throw out anyone that is not studying computer science related fields\n",
    "non_cs_students: list[str] = df_personal_information.query(\"CourseOfStudy != @possible_courses_of_study\")[\"CourseOfStudy\"].tolist()\n",
    "print(f'Have to drop {len(non_cs_students)} {non_cs_students} because they were not studying Computer Science')\n",
    "\n",
    "participants = df_personal_information['ID'].unique().tolist()\n",
    "\n",
    "# change columns to numeric value\n",
    "df_personal_information['Age'] = pd.to_numeric(df_personal_information['Age'])\n",
    "df_personal_information['Semester'] = pd.to_numeric(df_personal_information['Semester'])\n",
    "df_personal_information['YearsProgramming'] = pd.to_numeric(df_personal_information['YearsProgramming'])\n",
    "df_personal_information['Eyesight'] = df_personal_information['Eyesight'].astype(\"category\")\n",
    "df_personal_information['Gender'] = df_personal_information['Gender'].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness and Time\n",
    "\n",
    "I read in all data from the files containing the results (`ResultsOverall_{ID}`) and add the `ID` for the participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall: DataFrame = pd.DataFrame(columns=necessary_columns + resultsOverall)\n",
    "\n",
    "for i in range(0, len(participants)):\n",
    "\n",
    "\tdf_to_add_general: DataFrame = DataFrame()\n",
    "\n",
    "\t# check if the participant has finished all snippets and read in the results data\n",
    "\ttry:\n",
    "\t\tdf_to_add: DataFrame = pd.read_csv(f'./AllParticipants/{participants[i]}/Trial_{participants[i]}/ResultsOverall_{participants[i]}.csv', sep=';').assign(ID=participants[i])\n",
    "\n",
    "\t\tif df_to_add['Answer_Out'].isnull().values.any():\n",
    "\t\t\tprint(f'Participant {participants[i]} did not finish all snippets')\n",
    "\t\t\tdf_meta_data.loc[i, 'Finished'] = False\n",
    "\t\telse:\n",
    "\t\t\tdf_meta_data.loc[i, 'Finished'] = True\n",
    "\n",
    "\t\tdf_to_add['ID'] = participants[i]\n",
    "\t\tdf_results_overall = pd.concat([df_results_overall, df_to_add], ignore_index=True)\n",
    "\n",
    "\t\tdf_meta_data.loc[i, 'ResultsOverall'] = True\n",
    "\n",
    "\texcept:\n",
    "\t\tprint(f'Participant {participants[i]} has no ResultsOverall.csv file')\n",
    "\t\tdf_meta_data.loc[i, 'ResultsOverall'] = False\n",
    "\n",
    "# now get the values from the eyetracking and merge it into the dataframe\n",
    "df_eye_tracking_metrics: DataFrame = pd.read_csv(f'./eyetracking/output/AOI/Metrics_Data_for_Anova.csv', sep=';')\n",
    "df_results_overall['SubjectID'] = df_results_overall['SubjectID'].apply(lambda x: x[:8])\n",
    "df_results_overall = pd.merge(df_results_overall, df_eye_tracking_metrics, how='left', left_on=['SubjectID', 'Task'], right_on=['Participant', 'Snippet'])\n",
    "\n",
    "\n",
    "# sort the dataframe by the participant `ID` and `Number`\n",
    "df_results_overall.sort_values(by=['ID', 'Number'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the meaningful tag to the personal information dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning for Correctness and Time\n",
    "\n",
    "We can drop the column `SubjectID` and turn the `Time` column into seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnecessary_columns_overall_results: list[str] = [\n",
    "\t'SubjectID',\n",
    "\t'Number',\n",
    "\t'Participant',\n",
    "\t'Expert',\n",
    "\t'ExecOrder_Naive_Score',\n",
    "\t'ExecOrder_Dynamic_Score', \n",
    "\t'ExecOrder_Dynamic_Repetitions',\n",
    "\t'Snippet',\n",
    "]\n",
    "\n",
    "# remove unnecessary columns\n",
    "try:\n",
    "\tdf_results_overall.drop(columns=unnecessary_columns_overall_results, inplace=True, ignore_index=True)\n",
    "except:\n",
    "\tpass\n",
    "\n",
    "# Show whether a participant did all snippets or if they ran into a timeout\n",
    "for i in range(0, len(participants)):\n",
    "\tif df_results_overall.query(f'ID == \"{participants[i]}\"')['Answer_Out'].isnull().values.any():\n",
    "\t\tnumber_of_missing_snippets: int = df_results_overall.query(f'ID == \"{participants[i]}\"')[\"Answer_Out\"].isnull().sum()\n",
    "\t\tprint(f'Participant {participants[i]} did not finish all snippets, they missed {number_of_missing_snippets} snippets.')\n",
    "\t\tdf_meta_data.loc[i, 'NumberOfMissingSnippets'] = number_of_missing_snippets\n",
    "\telse:\n",
    "\t\tdf_meta_data.loc[i, 'NumberOfMissingSnippets'] = 0\n",
    "\n",
    "\tif df_results_overall.query(f'ID == \"{participants[i]}\"')['TimeOut'].any():\n",
    "\t\tprint(f'Participant {participants[i]} ran into a timeout.')\n",
    "\t\n",
    "\t\n",
    "\n",
    "# drop any line that has a `Time` value of 0\n",
    "df_results_overall: DataFrame = df_results_overall[df_results_overall['Time'] != 0]\n",
    "\n",
    "# drop every line that has a `Task` value of `WarmUp`\n",
    "df_results_overall: DataFrame = df_results_overall[df_results_overall['Task'] != 'WarmUp']\n",
    "\n",
    "# turn the `Time` column into seconds as it is currently in milliseconds\n",
    "df_results_overall['Time'] = df_results_overall['Time'].apply(lambda x: x // 1000 if x > 1000 else x)\n",
    "\n",
    "# fill the columns `Meaningful` and `Type Annotation` with the correct values\n",
    "df_results_overall['Meaningful'] = df_results_overall['Task'].apply(lambda x: True if x[-2] == 'M' else False)\n",
    "df_results_overall['TypeAnnotation'] = df_results_overall['Task'].apply(lambda x: True if x[-1] == 'T' else False)\n",
    "\n",
    "# set the column of Meaningful to True if the participant has at least one meaningful snippet\n",
    "df_meta_data['Meaningful'] = df_meta_data.apply(lambda x: True if df_results_overall.query(f'ID == \"{x[\"ID\"]}\"')['Meaningful'].any() else False, axis=1)\n",
    "\n",
    "# save which of the participants were assigned to the respective groups\n",
    "meaningful_participants = df_meta_data.query('Meaningful == True')['ID'].values\n",
    "meaningless_participants = df_meta_data.query('Meaningful == False')['ID'].values\n",
    "\n",
    "# remove the last two letters from the `Task` column\n",
    "df_results_overall['Task'] = df_results_overall['Task'].apply(lambda x: x[:-2] if x[-2] == 'M' or x[-2] == 'L' else x)\n",
    "\n",
    "# compute if the correct answer was given for each `Task` and `ID` and add it to `CorrectAnswer`\n",
    "df_results_overall['CorrectAnswer'] = df_results_overall.apply(lambda x: True if snippet_answer[x['Task']] == x['Answer_Out'] else False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge `df_results_overall` and `df_difficulty_rating` and also merge `personal_information` and the `Meaningful` columns of `df_results_overall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall = pd.merge(df_results_overall, df_difficulty_rating, how='left', left_on=['ID','Task'], right_on=['ID','Task'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computations for Correctness and Time\n",
    "\n",
    "First, we create a general overview of the tables data.\n",
    "\n",
    "We compute the time taken for each `Task` for each participant `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_computation: dict[str, str] = {\n",
    "    'CorrectAnswer': [np.count_nonzero,],\n",
    "    'TypeAnnotation': ['count',],\n",
    "    'Time': ['mean', 'median', 'min', 'max', 'sum',],\n",
    "}\n",
    "\n",
    "overview_correctness_time: DataFrame = df_results_overall.groupby(['ID', 'Meaningful', 'TypeAnnotation']).agg(aggregation_computation).reset_index()\n",
    "\n",
    "overview_correctness_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the meaningful and meaningless values to the personal information DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal_information['Meaningful'] = df_personal_information['ID'].apply(lambda x: True if x in meaningful_participants else False)\n",
    "\n",
    "df_personal_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall.groupby(['ID', 'Task']).agg({'Time': 'mean'}).unstack().T.fillna('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove certain data points from response time. These are the highest 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the outlier from the response time\n",
    "print(f'95% quantile = {df_results_overall[\"Time\"].quantile(0.95)} seconds')\n",
    "print(f'5% quantile = {df_results_overall[\"Time\"].quantile(0.05)} seconds')\n",
    "\n",
    "q = df_results_overall['Time'].quantile(0.95)\n",
    "\n",
    "df_results_overall = df_results_overall[df_results_overall['Time'] < q]\n",
    "print(df_results_overall['Time'].describe())\n",
    "\n",
    "# completely remove participant 31195\n",
    "df_results_overall = df_results_overall[df_results_overall['ID'] != '31195']\n",
    "df_personal_information = df_personal_information[df_personal_information['ID'] != '31195']\n",
    "df_meta_data = df_meta_data[df_meta_data['ID'] != '31195']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_results_overall['ID'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_personal_information.CourseOfStudy.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(df_personal_information, x='CourseOfStudy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People Distributions and Data\n",
    "\n",
    "1. The age\n",
    "2. Their experience\n",
    "3. Their Course of Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_personal_information, y='Age',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_personal_information, y='Age', hue='Meaningful', gap=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The courses of study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_personal_information, x='CourseOfStudy',hue='Gender')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of semesters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df_personal_information, x='CourseOfStudy',hue='Gender', y='Semester')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of semesters in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_personal_information, y='Semester', hue='Meaningful', gap=.1,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classmates and overall experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_personal_information, y='OverallExperience', x='Semester', hue='Meaningful', gap=.1, \n",
    "            # order=['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced']\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classmates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_personal_information, \n",
    "            y='Classmates', \n",
    "            x='Semester', \n",
    "            hue='Meaningful', \n",
    "            gap=.1, \n",
    "            # order=['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'],\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverallExperience by ProgrammingLately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=df_personal_information,\n",
    "            x='ProgrammingLately',\n",
    "            y='Classmates',\n",
    "            hue='Meaningful',\n",
    "            # gap=.1,\n",
    "            order=['Daily', 'Weekly', 'Monthly', 'Yearly'],\n",
    "            kind='swarm'\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Data for the personal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Age:\\n{df_personal_information[\"Age\"].describe()}\\n')\n",
    "print(f'Semester:\\n{df_personal_information[\"Semester\"].describe()}\\n')\n",
    "print(f'YearsProgramming:\\n{df_personal_information[\"YearsProgramming\"].describe()}\\n')\n",
    "print(f'Gender:\\n{df_personal_information.groupby(\"Gender\").agg({\"Gender\": \"count\"})}\\n')\n",
    "print(f'Classmates:\\n{df_personal_information.groupby(\"Classmates\").agg({\"Classmates\": \"count\"})}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Data for personal information divided by Meaningful Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Age:\\n{df_personal_information.groupby(\"Meaningful\").agg({\"Age\": [\"mean\", \"std\"]})}')\n",
    "print(f'Semester:\\n{df_personal_information.groupby(\"Meaningful\").agg({\"Semester\": [\"mean\", \"std\"]})}\\n')\n",
    "print(f'YearsProgramming:\\n{df_personal_information.groupby(\"Meaningful\").agg({\"YearsProgramming\": [\"mean\", \"std\"]})}\\n')\n",
    "print(f'Gender:\\n{df_personal_information.groupby([\"Meaningful\", \"Gender\"]).agg({\"Gender\": \"count\"})}\\n')\n",
    "print(f'EyeSight:\\n{df_personal_information.groupby([\"Meaningful\", \"Eyesight\"]).agg({\"Eyesight\": \"count\"})}\\n')\n",
    "print(f'StudyBefore:\\n{df_personal_information.groupby([\"Meaningful\", \"StudyBefore\"]).agg({\"StudyBefore\": \"count\"})}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficacy for every participant:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results_overall.groupby(['ID', 'TypeAnnotation']).agg({'Time': 'sum', 'CorrectAnswer': 'count'})\n",
    "df['efficacy'] = df.CorrectAnswer / (df.Time / 60) \n",
    "# df.columns.droplevel(0)\n",
    "# print(df)\n",
    "for index, row in df.iterrows():\n",
    "    print(f'Efficacy of Participant {index}: {row.CorrectAnswer / (row.Time / 60)}')\n",
    "\n",
    "sns.violinplot(df, y='efficacy', hue='TypeAnnotation', cut=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Methods one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "def compute_statistics(data: DataFrame, with_TA, without_TA, input_type: str = 'continuous', output_type: str = 'contiuous', population: str = 'within', p_value: float = 0.05):\n",
    "\n",
    "    # input_dependence: str = 'independent',\n",
    "\n",
    "    # check for normality\n",
    "    with_TA_normality = stats.shapiro(with_TA)\n",
    "    without_TA_normality = stats.shapiro(without_TA)\n",
    "\n",
    "    # print(with_TA)\n",
    "    # print(without_TA)\n",
    "\n",
    "    # check for homogeneity of variance\n",
    "    homogeneity_of_variance = stats.levene(with_TA, without_TA)\n",
    "    # print(homogeneity_of_variance)\n",
    "\n",
    "    if with_TA_normality.pvalue > p_value and without_TA_normality.pvalue > p_value:\n",
    "        print(f'Both groups are normally distributed with pvalues {with_TA_normality.pvalue} and {without_TA_normality.pvalue}')\n",
    "    else:\n",
    "        print(f'At least one group is not normally distributed with pvalues {with_TA_normality.pvalue} and {without_TA_normality.pvalue}')\n",
    "\n",
    "    if homogeneity_of_variance.pvalue > p_value:\n",
    "        print(f'Both groups have homogeneity of variance with pvalue {homogeneity_of_variance.pvalue}')\n",
    "    else:\n",
    "        print(f'Both groups do not have homogeneity of variance with pvalue {homogeneity_of_variance.pvalue}')\n",
    "\n",
    "    if with_TA_normality.pvalue > p_value and without_TA_normality.pvalue > p_value and homogeneity_of_variance.pvalue > p_value:\n",
    "        if population == 'within':\n",
    "            if input_type == 'categorical' and output_type == 'continuous':\n",
    "                print(f'TTEST_REL: {stats.ttest_rel(without_TA, with_TA)}')\n",
    "            elif input_type == 'categorical' and output_type == 'categorical':\n",
    "                print(f'CHISQUARE: {stats.chisquare(without_TA, with_TA)}')\n",
    "            elif len(with_TA) == len(without_TA):\n",
    "                print(f'WILCOXON TWO-SIDED: {stats.wilcoxon(without_TA, with_TA)}')\n",
    "                print(f'WILCOXON ONE-SIDED Greater: {stats.wilcoxon(without_TA, with_TA, alternative=\"greater\")}')\n",
    "                print(f'WILCOXON ONE-SIDED Less: {stats.wilcoxon(without_TA, with_TA, alternative=\"less\")}')\n",
    "            else:\n",
    "                print(f'MANNWHITNEYU: {stats.mannwhitneyu(without_TA, with_TA)}')\n",
    "\n",
    "        else:\n",
    "            if input_type == 'categorical' and output_type == 'continuous':\n",
    "                print(f'TTEST_IND: {stats.ttest_ind(without_TA, with_TA)}')\n",
    "            else:\n",
    "                print(f'MANNWHITNEYU: {stats.mannwhitneyu(without_TA, with_TA)}')\n",
    "\n",
    "    else:\n",
    "        if population == 'within' and len(with_TA) == len(without_TA):\n",
    "            print(f'WILCOXON TWO-SIDED: {stats.wilcoxon(without_TA, with_TA)}')\n",
    "        elif input_type == 'categorical' and output_type == 'categorical':\n",
    "            print(f'CHISQUARE: {stats.chi2_contingency(without_TA, with_TA)}')\n",
    "        else:\n",
    "            u_statistic, p_value = stats.mannwhitneyu(without_TA, with_TA)\n",
    "            print(f'The U-statistic is {u_statistic} and the p-value is {p_value}')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table for Type Annotations and Correctness RQ 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do everything correctly for time\n",
    "\n",
    "1. Check for normality\n",
    "2. Check for the variances\n",
    "3. then check for wilcoxon two-sided\n",
    "4. then check for wilcoxon one-sided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data: dict[str, list[str]|str] = {\n",
    "    'Time': ['mean', 'std'],\n",
    "    'CorrectAnswer': ['sum', 'count'],\n",
    "}\n",
    "\n",
    "table: DataFrame = df_results_overall.groupby(['Task']).agg(table_data)\n",
    "\n",
    "TA_mean = df_results_overall.query('TypeAnnotation == True')['Time']\n",
    "NoTA_mean = df_results_overall.query('TypeAnnotation == False')['Time']\n",
    "\n",
    "# qq = stats.probplot(TA_mean, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Normal Q-Q plot\")\n",
    "# plt.show()\n",
    "\n",
    "data = compute_statistics(table, TA_mean, NoTA_mean, input_type='categorical', output_type='continuous', population='within')\n",
    "\n",
    "# print(f'{TA_mean.describe()} and {NoTA_mean.describe()}')\n",
    "\n",
    "# qq = stats.probplot(NoTA_mean, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Normal Q-Q plot\")\n",
    "# plt.show()\n",
    "\n",
    "# print(f'Normality of TA:\\n{stats.shapiro(TA_mean)}')\n",
    "# print(f'Normality of NoTA:\\n{stats.shapiro(NoTA_mean)}')\n",
    "\n",
    "# print(f'Levene Test:\\n{stats.levene(TA_mean, NoTA_mean)}')\n",
    "\n",
    "# print(f'WILCOXON:\\n{stats.wilcoxon(NoTA_mean, TA_mean)}')\n",
    "# print(f'WILCOXON Longer with TA:\\n{stats.wilcoxon(NoTA_mean, TA_mean, alternative=\"greater\")}')\n",
    "# print(f'WILCOXON Shorter with TA:\\n{stats.wilcoxon(NoTA_mean, TA_mean, alternative=\"less\")}')\n",
    "for task_tuple, x in table.iterrows():\n",
    "    print(f'{task_tuple} & {x.CorrectAnswer[\"sum\"]}/{x.CorrectAnswer[\"count\"]} ({round((x.CorrectAnswer[\"sum\"]/x.CorrectAnswer[\"count\"])*100)}\\%) & {round(x.Time[\"mean\"],2)} $\\pm$ {round(x.Time[\"std\"],2)}\\\\\\\\')\n",
    "\n",
    "print(f'{df_results_overall.Time.mean()} + {df_results_overall.Time.std()} | {df_results_overall.CorrectAnswer.sum()}/{df_results_overall.CorrectAnswer.count()} ({df_results_overall.CorrectAnswer.sum()/df_results_overall.CorrectAnswer.count()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_results_overall, y='Time', x='TypeAnnotation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above for the correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_TA = table.query(\"TypeAnnotation == True\")[\"Correctness\"]\n",
    "# percentage_NoTA = table.query(\"TypeAnnotation == False\")[\"Correctness\"]\n",
    "\n",
    "# please create a contingency table for CorrectAnswer on TypeAnnotation\n",
    "\n",
    "contingency_table = pd.crosstab(df_results_overall['TypeAnnotation'], df_results_overall['CorrectAnswer'])\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)\n",
    "\n",
    "try:\n",
    "    data = compute_statistics(DataFrame(), df_results_overall.query(\"TypeAnnotation == True\")[\"CorrectAnswer\"], df_results_overall.query(\"TypeAnnotation == False\")[\"CorrectAnswer\"], input_type='categorical', output_type='categorical', population='within')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(df_results_overall.groupby('TypeAnnotation')['CorrectAnswer'].agg(['sum', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures for RQ 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should check for VerticalNext, VerticalLater, Regression, HorizontalLater, and LineRegression\n",
    "\n",
    "But first let's create the table to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_measure: list[str] = ['VerticalNext', 'VerticalLater', 'Regression', 'HorizontalLater', 'LineRegression', 'StoryOrder_Naive_Score','StoryOrder_Dynamic_Score','StoryOrder_Dynamic_Repetitions', 'SaccadeLength']\n",
    "\n",
    "table: DataFrame = df_results_overall.groupby(['Task', 'TypeAnnotation']).agg(table_data)\n",
    "table.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure in list_measure:\n",
    "    print(f'\\nMeasure: {measure}')\n",
    "    data = compute_statistics(table, df_results_overall.query(\"TypeAnnotation == True\")[measure].dropna(), df_results_overall.query(\"TypeAnnotation == False\")[measure].dropna(), input_type='categorical', output_type='continuous', population='within')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures for RQ 1.3\n",
    "\n",
    "This should include:\n",
    "\n",
    "1. The difficulty for each of the snippets as giving by the people.\n",
    "2. Did the Type Annotations help with the snippets?\n",
    "3. Do Type Annotations help in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a likert scale analysis of the difficulty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_results_overall.groupby('TypeAnnotation')['Difficulty'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Difficulty', \n",
    "            hue='TypeAnnotation', \n",
    "            data=df1, \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            # legend=False,\n",
    ")\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "# plt.title('Difficulty grouped by Type Annotation')\n",
    "# plt.legend(title='Type Annotation', loc='upper right', labels=['Non-Annotated', 'Annotated'])\n",
    "plt.savefig(f'{figure_path}/rq13difficultyLikertCatPlot.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_results_overall.groupby(['Meaningful', 'TypeAnnotation'])['Difficulty'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "print(df1[df1['Meaningful'] == True])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Difficulty', \n",
    "            hue='TypeAnnotation', \n",
    "            data=df1[df1['Meaningful'] == True], \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    ")\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_results_overall.groupby(['Meaningful', 'TypeAnnotation'])['Difficulty'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# g = sns.FacetGrid(df1, col='Meaningful', hue='TypeAnnotation')\n",
    "# g.map(sns.stripplot, 'Difficulty', 'percent', kind='bar')\n",
    "sns.catplot(x='Difficulty', \n",
    "            hue='TypeAnnotation', \n",
    "            data=df1, \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            col='Meaningful',\n",
    ")\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotCombined.pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Difficulty', \n",
    "            hue='TypeAnnotation', \n",
    "            data=df1[df1['Meaningful'] == False], \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    ")\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "plt.title('Difficulty grouped by Type Annotation')\n",
    "plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotObfuscated.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we also find a statistical significance for this? using the chisquare test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df_results_overall['TypeAnnotation'], df_results_overall['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df = df_results_overall[df_results_overall['Meaningful'] == True]\n",
    "contingency_table = pd.crosstab(cont_df['TypeAnnotation'], cont_df['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_df = df_results_overall[df_results_overall['Meaningful'] == False]\n",
    "contingency_table = pd.crosstab(cont_df['TypeAnnotation'], cont_df['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df_results_overall['Meaningful'], df_results_overall['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about some significance for the Comprehension within the study?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df_personal_information['TAComprehension'], df_personal_information['TAComprehensionGeneral'])\n",
    "\n",
    "print(stats.fisher_exact(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df_personal_information['TAComprehension'], df_personal_information['Meaningful'])\n",
    "\n",
    "print(stats.fisher_exact(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(df_personal_information['TAComprehensionGeneral'], df_personal_information['Meaningful'])\n",
    "\n",
    "print(stats.fisher_exact(contingency_table))\n",
    "print(contingency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the TA help with comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data: dict[str|list[str]] = {\n",
    "    # 'Difficulty': ['mean'],\n",
    "    'TAComprehension': [],\n",
    "    # 'TAComprehensionGeneral': [],\n",
    "}\n",
    "print(df_personal_information.query('TAComprehension == False')['ID'])\n",
    "df_personal_information['TAComprehension'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TA help with comprehension for Meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal_information.groupby('Meaningful')['TAComprehension'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chisquare(df_personal_information.groupby('Meaningful')['TAComprehension'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do TA help in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_personal_information.query('TAComprehensionGeneral == False')['ID'])\n",
    "\n",
    "df_personal_information['TAComprehensionGeneral'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_personal_information.groupby('Meaningful')['TAComprehensionGeneral'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the plot for the difficulty of the snippets grouped by meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_results_overall.query('Meaningful == True'), x='Task', y=\"Difficulty\", hue=\"TypeAnnotation\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "plt.savefig(f'{figure_path}/difficultyrq23_Meaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_results_overall.query('Meaningful == False'), x='Task', y=\"Difficulty\", hue=\"TypeAnnotation\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "plt.savefig(f'{figure_path}/difficultyrq23_Obfuscated.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures for RQ 2.1\n",
    "\n",
    "This shall include all information for the bahavioral measures as seen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data: dict[str, list[str]|str] = {\n",
    "    'Time': ['mean', 'std'],\n",
    "    # 'CorrectAnswer': ['sum', 'count'],\n",
    "}\n",
    "\n",
    "table: DataFrame = df_results_overall.groupby(['Task', 'TypeAnnotation', 'Meaningful']).agg(table_data)\n",
    "\n",
    "# TA_L_mean = table.query(\"TypeAnnotation == True and Meaningful == False\")[\"Time\"][\"mean\"]\n",
    "# NoTA_L_mean = table.query(\"TypeAnnotation == False and Meaningful == False\")[\"Time\"][\"mean\"]\n",
    "# TA_M_mean = table.query('TypeAnnotation == True and Meaningful == True')['Time']['mean']\n",
    "# NoTA_M_mean = table.query('TypeAnnotation == False and Meaningful == True')['Time']['mean']\n",
    "\n",
    "TA_L = df_results_overall.query(\"TypeAnnotation == True and Meaningful == False\")[\"Time\"]\n",
    "NoTA_L = df_results_overall.query(\"TypeAnnotation == False and Meaningful == False\")[\"Time\"]\n",
    "TA_M = df_results_overall.query(\"TypeAnnotation == True and Meaningful == True\")[\"Time\"]\n",
    "NoTA_M = df_results_overall.query(\"TypeAnnotation == False and Meaningful == True\")[\"Time\"]\n",
    "\n",
    "print(f'NORMALITY:\\n{pg.normality(df_results_overall, dv=\"Time\", group=\"TypeAnnotation\")}')\n",
    "print(f'NORMALITY:\\n{pg.normality(df_results_overall, dv=\"Time\", group=\"Meaningful\")}')\n",
    "\n",
    "\n",
    "print(f'Levene:\\n{stats.levene(TA_L, NoTA_M, TA_M, NoTA_L)}')\n",
    "\n",
    "print(f'{df_results_overall.query(\"Time == 0\")}')\n",
    "\n",
    "# print(f'Obfuscated: Wilcoxon NoTA -> TA:\\n{stats.wilcoxon(NoTA_L_mean, TA_L_mean)}')\n",
    "# print(f'Obfuscated: MannWhitneyU NoTA -> Meaningful NoTA:\\n{stats.mannwhitneyu(NoTA_L_mean, NoTA_M_mean)}')\n",
    "# print(f'Obfuscated: MannWhitneyU NoTA -> Meaningful TA:\\n{stats.mannwhitneyu(NoTA_L_mean, TA_M_mean)}')\n",
    "# print(f'Obfuscated: MannWhitneyU TA -> Meaningful TA:\\n{stats.mannwhitneyu(TA_L_mean, TA_M_mean)}')\n",
    "# print(f'Meaningful: MannWhitneyU NoTA -> Obfuscated TA:\\n{stats.mannwhitneyu(NoTA_M_mean, TA_L_mean)}')\n",
    "# print(f'Meaningful: Wilcoxon NoTA -> TA:\\n{stats.wilcoxon(NoTA_M_mean, TA_M_mean)}')\n",
    "print(pg.sphericity(df_results_overall, dv='Time', within=['Meaningful'], subject='ID'))\n",
    "print(pg.sphericity(df_results_overall, dv='Time', within=['TypeAnnotation'], subject='ID'))\n",
    "\n",
    "\n",
    "\n",
    "print(pg.sphericity(df_results_overall, dv='CorrectAnswer', within=['Meaningful'], subject='ID'))\n",
    "print(pg.sphericity(df_results_overall, dv='CorrectAnswer', within=['TypeAnnotation'], subject='ID'))\n",
    "# print(pg.sphericity(df_results_overall, dv='Time', within=['TypeAnnotation', 'Meaningful'], subject='ID',))\n",
    "# (df_results_overall[['Time', 'TypeAnnotation', 'Meaningful']].corr())#.sum(1).sort_values(ascending=False)\n",
    "# print(stats.bartlett())\n",
    "\n",
    "\n",
    "\n",
    "# piv = df_results_overall.pivot(index='ID', columns=['Meaningful', 'TypeAnnotation'], values='Time')\n",
    "# piv.head()\n",
    "# print(pg.sphericity(piv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pymer4 to analyze the time with variables annotation and meaningful\n",
    "model = pymer4.Lmer('Time ~ TypeAnnotation * Meaningful + (1|Task)', data=df_results_overall).fit()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for contingency in CorrectAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall['CorrectAnswerNumeric'] = df_results_overall['CorrectAnswer'].apply(lambda x: 1 if x else 0)\n",
    "model = smf.logit('CorrectAnswerNumeric ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall, groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())\n",
    "print(model.wald_test_terms(scalar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.mixedlm('Time ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall, groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())\n",
    "print(model.wald_test_terms(scalar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RQ 2.2 Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_measure: list[str] = ['VerticalNext', \n",
    "                           'VerticalLater', 'Regression', 'HorizontalLater', 'LineRegression', 'StoryOrder_Naive_Score','StoryOrder_Dynamic_Score','StoryOrder_Dynamic_Repetitions', 'SaccadeLength', 'Linearity'\n",
    "                           ]\n",
    "\n",
    "df_results_overall = df_results_overall[df_results_overall['LineRegression'].notna()].reset_index()\n",
    "df_results_overall = df_results_overall[df_results_overall['SaccadeLength'].notna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure in list_measure:\n",
    "    print(f'\\nMeasure: {measure}')\n",
    "    if measure not in []:\n",
    "        model = smf.mixedlm(f'{measure} ~ C(TypeAnnotation) * C(Meaningful)', data=df_results_overall, groups=df_results_overall['Task']).fit()\n",
    "        print(model.summary())\n",
    "    else:\n",
    "        model = smf.mixedlm(f'{measure} ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall, groups=df_results_overall['Task']).fit()\n",
    "    print(model.wald_test_terms(scalar=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RQ 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results_overall['DifficultyNumeric'] = df_results_overall['Difficulty'].apply(lambda x: 1 if x == 'Very Difficult' else  0)\n",
    "df_results_overall['DifficultyNumeric'] = df_results_overall['Difficulty'].dropna().astype(float)\n",
    "df_results_overall['DifficultyNumeric'] = df_results_overall['DifficultyNumeric'].apply(lambda x: x / 5)\n",
    "model = smf.logit('DifficultyNumeric ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall, groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())\n",
    "print(model.wald_test_terms(scalar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAComprehension influenced by Meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 = df_results_overall.Time.quantile(0.75)\n",
    "Q1 = df_results_overall.Time.quantile(0.25)\n",
    "print(df_results_overall.Time.mean())\n",
    "print(df_results_overall.Time.std())\n",
    "\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "threshold = 1.5\n",
    "\n",
    "outliers = df_results_overall[(df_results_overall['Time'] < Q1 - threshold * IQR) | (df_results_overall['Time'] > Q3 + threshold * IQR)]\n",
    "print(f'{len(outliers.Time)}, {len(df_results_overall.Time)}')\n",
    "sns.boxplot(df_results_overall, y='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous\n",
    "\n",
    "How did the people find themselves in comparison to others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_personal_information.Classmates.value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Classmates', \n",
    "            # hue='TypeAnnotation', \n",
    "            data=df1, \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    ")\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OverallExperience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_personal_information.OverallExperience.value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='OverallExperience', \n",
    "            # hue='TypeAnnotation', \n",
    "            data=df1, \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    ")\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Years Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df_personal_information.OverallExperience.value_counts(normalize=True)\n",
    "# df1 = df1.mul(100)\n",
    "# df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "# print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='YearsProgramming',\n",
    "            # hue='TypeAnnotation', \n",
    "            data=df_personal_information, \n",
    "            # kind='bar',\n",
    ")\n",
    "\n",
    "# plt.xticks(np.arange(5), ['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall.Linearity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Snippets with their mean and SD with correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_results_overall.groupby(['Task', 'TypeAnnotation']).agg({'Time': ['mean', 'std']})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently unnecessary and unused data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_results_overall.groupby(['TypeAnnotation', 'Meaningful'])['CorrectAnswer'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Meaningful', \n",
    "            hue='TypeAnnotation', \n",
    "            data=df1, \n",
    "            kind='bar',\n",
    "            y='percent',\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table for Type Annotations and Meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data: dict[str, list[str]|str] = {\n",
    "    'Time': ['mean', 'std'],\n",
    "    'CorrectAnswer': ['sum', 'count'],\n",
    "}\n",
    "\n",
    "df_results_overall.groupby(['Task', 'Meaningful', 'TypeAnnotation']).agg(table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot for the mean length of each task\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='Task', y='Time', data=df_results_overall)\n",
    "plt.title('Mean Time per Task')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean time per task by Type Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot that shows the mean time for each task\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Mean Time per Task')\n",
    "\n",
    "axs[0].boxplot(x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == True')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0].set_title('Mean Time per Task and True Type Annotation')\n",
    "\n",
    "\n",
    "axs[1].boxplot(x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == False')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1].set_title('Mean Time per Task and False Type Annotation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean time per task by Identifier Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot that shows the mean time for each task\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Mean Time per Task')\n",
    "\n",
    "axs[0].boxplot(x=[df_results_overall.query(f'Task == @task and `Meaningful` == True')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0].set_title('Mean Time per Task and Meaningful Identifier Names')\n",
    "\n",
    "\n",
    "axs[1].boxplot(x=[df_results_overall.query(f'Task == @task and `Meaningful` == False')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1].set_title('Mean Time per Task and Meaningless Identifier Names')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean time per task and for the combinations of Type Annoations and Identifier Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot that shows the mean time for each task\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Mean Time per Task')\n",
    "\n",
    "axs[0, 0].boxplot(x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == False and `Meaningful` == True')['Time'] for task in all_tasks], labels=all_tasks, notch=False)\n",
    "axs[0, 0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0, 0].set_title('Mean Time per Task without Type Annotations and with Meaningful Identifier Names')\n",
    "\n",
    "axs[0, 1].boxplot(x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == True and `Meaningful` == True')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[0, 1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0, 1].set_title('Mean Time per Task with Type Annotations and with Meaningful Identifier Names')\n",
    "\n",
    "axs[1, 0].boxplot(x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == False and `Meaningful` == False')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[1, 0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1, 0].set_title('Mean Time per Task without Type Annotations and with Meaningless Identifier Names')\n",
    "\n",
    "axs[1, 1].boxplot(x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == True and `Meaningful` == False')['Time'] for task in all_tasks], labels=all_tasks,)\n",
    "axs[1, 1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1, 1].set_title('Mean Time per Task with Type Annotations and with Meaningless Identifier Names')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot to describe how long each task took overall and how this is distributed among the participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot that shows how long each participant took grouped by task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the Type Annotation plots in the top row into one bigger plot with comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(df_results_overall.query(f'`Meaningful` == True'), x='Task', y='Time', hue='TypeAnnotation', split=True, gap=.1, inner=\"quart\", cut=0, order=all_tasks)\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.title('Mean Time per Task with Meaningful Identifier Names')\n",
    "plt.savefig(f'{figure_path}/timePerTaskrq21_meaningful.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(df_results_overall.query(f'`Meaningful` == False'), x='Task', y='Time', hue='TypeAnnotation', split=True, gap=.1, inner=\"quart\", cut=0, order=all_tasks)\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.title('Mean Time per Task with Meaningless Identifier Names') \n",
    "\n",
    "plt.savefig(f'{figure_path}/timePerTaskrq21_obfuscated.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What were the Means and the Standard Deviation for meaningful and type annotation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results_overall.groupby(['Meaningful', 'TypeAnnotation', ]).agg({'Time': ['mean', 'std']}))\n",
    "print(df_results_overall.groupby(['Meaningful']).agg({'Time': ['mean', 'std']}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results_overall.groupby(['Meaningful', 'TypeAnnotation', ]).agg({'CorrectAnswer': ['sum', 'count']}))\n",
    "print(df_results_overall.groupby(['Meaningful']).agg({'CorrectAnswer': ['sum', 'count']}))\n",
    "print(df_results_overall.groupby(['TypeAnnotation']).agg({'CorrectAnswer': ['sum', 'count']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation heatmap and p-values for the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation_meantimes_snippets: DataFrame = DataFrame()\n",
    "\n",
    "df_correlation_meantimes_snippets['M-TA'] = df_results_overall.query('TypeAnnotation == True and Meaningful == True').groupby('Task').agg({'Time': 'mean'})['Time']\n",
    "df_correlation_meantimes_snippets['L-N'] = df_results_overall.query('TypeAnnotation == False and Meaningful == False').groupby('Task').agg({'Time': 'mean'})['Time']\n",
    "df_correlation_meantimes_snippets['L-TA'] = df_results_overall.query('TypeAnnotation == True and Meaningful == False').groupby('Task').agg({'Time': 'mean'})['Time']\n",
    "df_correlation_meantimes_snippets['M-N'] = df_results_overall.query('TypeAnnotation == False and Meaningful == True').groupby('Task').agg({'Time': 'mean'})['Time']\n",
    "\n",
    "matrix = df_correlation_meantimes_snippets.corr()\n",
    "print(f'P-Values:\\n{calculate_pvalues(df_correlation_meantimes_snippets)}')\n",
    "\n",
    "sns.heatmap(matrix, cmap=\"Greens\", annot=True)\n",
    "plt.title(f'Correlation Mean-Times')\n",
    "plt.savefig(f'{figure_path}/correlation_meantimesrq21.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and p-values for the correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlation_correctness_snippets: DataFrame = DataFrame()\n",
    "\n",
    "df_correlation_correctness_snippets['M-TA'] = df_results_overall.query('TypeAnnotation == True and Meaningful == True').groupby('Task').agg({'CorrectAnswer': 'sum'})['CorrectAnswer'] / df_results_overall.query('TypeAnnotation == True and Meaningful == True').groupby('Task').agg({'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "df_correlation_correctness_snippets['L-N'] = df_results_overall.query('TypeAnnotation == False and Meaningful == False').groupby('Task').agg({'CorrectAnswer': 'sum'})['CorrectAnswer'] / df_results_overall.query('TypeAnnotation == False and Meaningful == False').groupby('Task').agg({'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "df_correlation_correctness_snippets['L-TA'] = df_results_overall.query('TypeAnnotation == True and Meaningful == False').groupby('Task').agg({'CorrectAnswer': 'sum'})['CorrectAnswer'] / df_results_overall.query('TypeAnnotation == True and Meaningful == False').groupby('Task').agg({'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "df_correlation_correctness_snippets['M-N'] = df_results_overall.query('TypeAnnotation == False and Meaningful == True').groupby('Task').agg({'CorrectAnswer': 'sum'})['CorrectAnswer'] / df_results_overall.query('TypeAnnotation == False and Meaningful == True').groupby('Task').agg({'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "\n",
    "matrix = df_correlation_correctness_snippets.corr()\n",
    "print(f'P-Values:\\n{calculate_pvalues(df_correlation_correctness_snippets)}')\n",
    "\n",
    "sns.heatmap(matrix, cmap=\"Greens\", annot=True)\n",
    "plt.title(f'Correlation Correctness')\n",
    "plt.savefig(f'{figure_path}/correlation_correctnessrq21.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the mean time with and without type annotations for each participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# first plot\n",
    "plot_L_M = axs[0].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == True')['Time'] for participant in meaningful_participants], labels=meaningful_participants, positions=np.arange(len(meaningful_participants))*2.0+0.35, widths=0.6)\n",
    "plot_TA_M = axs[0].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == True')['Time'] for participant in meaningful_participants], labels=meaningful_participants, positions=np.arange(len(meaningful_participants))*2.0-0.35, widths=0.6)\n",
    "\n",
    "# first plot settings\n",
    "axs[0].set_title('Mean Time per Participant with Meaningful Identifier Names')\n",
    "axs[0].set_xticks(np.arange(0, len(meaningful_participants) * 2, 2), meaningful_participants, rotation=90)\n",
    "define_box_properties(plot_TA_M, 'blue', 'TypeAnnotation', axs[0])\n",
    "define_box_properties(plot_L_M, 'orange', 'No Type Annotation', axs[0])\n",
    "\n",
    "# second plot\n",
    "plot_L_L = axs[1].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == False')['Time'] for participant in meaningless_participants], labels=meaningless_participants, positions=np.arange(len(meaningless_participants))*2.0+0.35, widths=0.6)\n",
    "plot_TA_L = axs[1].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == False')['Time'] for participant in meaningless_participants], labels=meaningless_participants, positions=np.arange(len(meaningless_participants))*2.0-0.35, widths=0.6)\n",
    "\n",
    "# second plot settings\n",
    "axs[1].set_title('Mean Time per Participant with Meaningless Identifier Names')\n",
    "axs[1].set_xticks(np.arange(0, len(meaningless_participants) * 2, 2), meaningless_participants, rotation=90)\n",
    "define_box_properties(plot_TA_L, 'blue', 'TypeAnnotation', axs[1])\n",
    "define_box_properties(plot_L_L, 'orange', 'No Type Annotation', axs[1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long people took overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall.groupby(['Meaningful',]).agg({'Time': 'sum'}).unstack().T.plot(kind='bar', stacked=True, figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `meaningful` snippets make participants faster on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute if meaningful snippets were faster on average than meaningless snippets\n",
    "df_results_overall.groupby(['Meaningful']).agg({'Time': 'mean'}).unstack().T.plot(kind='bar', stacked=False, figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `Type Annotations` make people faster on average?\n",
    "\n",
    "- [ ] TODO: Apparently, this is not the case. Why could this be? More to read? Where do people look during this time? What are they focusing on? Are they more correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute if type annotated snippets were faster on average than non type annotated snippets\n",
    "df_results_overall.groupby(['TypeAnnotation']).agg({'Time': 'mean'}).unstack().T.plot(kind='bar', stacked=True, figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `Type Annotations` make people faster when we differentiate between `Meaningful` snippets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall.groupby(['Meaningful', 'TypeAnnotation']).agg({'Time': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are snippets with `Type Annotations` more correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippets_grouping: list[str|list[str]] = ['TypeAnnotation', 'Meaningful',  ['TypeAnnotation', 'Meaningful'],]\n",
    "\n",
    "# create a plot consisting of len(snippets_grouping) subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, x in enumerate(snippets_grouping):\n",
    "    ax = df_results_overall.groupby(x).agg({'CorrectAnswer': np.count_nonzero}).unstack().T.plot(kind='bar', ax=axs[i // 2, i % 2], title=(lambda x: f'{x[0]} and {x[1]}' if len(x) == 2 else x)(x), ylabel='Number of Correct Answers', legend=False)\n",
    "\n",
    "    ax.set_xticklabels(['False', 'True'], rotation=0)\n",
    "\n",
    "    if len(x) != 2:\n",
    "        ax.axhline(y=df_results_overall.groupby(x).agg({'CorrectAnswer': 'count'}).unstack()[0], color='orange', linestyle='--', label='Max False') # False\n",
    "        ax.axhline(y=df_results_overall.groupby(x).agg({'CorrectAnswer': 'count'}).unstack()[1], color='green', linestyle='--', label='Max True') # True\n",
    "        ax.legend(loc='lower center')\n",
    "    \n",
    "    for line in ax.lines:\n",
    "        ax.annotate(str(int(line.get_ydata()[0])), xy=(line.get_xdata()[0], line.get_ydata()[0]), xytext=(280, 2), textcoords='offset points', ha='right', va='bottom', fontsize=8)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are snippets that are `Meaningful` more correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_overall.groupby(['TypeAnnotation', 'Meaningful']).agg({'CorrectAnswer': 'count'}).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better representation of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_TA_M: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == True')['CorrectAnswer'].sum()\n",
    "correct_TA_L: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == False')['CorrectAnswer'].sum()\n",
    "correct_L_M: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == True')['CorrectAnswer'].sum()\n",
    "correct_L_L: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == False')['CorrectAnswer'].sum()\n",
    "\n",
    "total_TA_M: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == True')['CorrectAnswer'].count()\n",
    "total_TA_L: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == False')['CorrectAnswer'].count()\n",
    "total_L_M: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == True')['CorrectAnswer'].count()\n",
    "total_L_L: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == False')['CorrectAnswer'].count()\n",
    "\n",
    "print(f'Type Annotation and Meaningful: {correct_TA_M}/{total_TA_M} = {correct_TA_M / total_TA_M}')\n",
    "print(f'Type Annotation and Meaningless: {correct_TA_L}/{total_TA_L} = {correct_TA_L / total_TA_L}')\n",
    "print(f'No Type Annotation and Meaningful: {correct_L_M}/{total_L_M} = {correct_L_M / total_L_M}')\n",
    "print(f'No Type Annotation and Meaningless: {correct_L_L}/{total_L_L} = {correct_L_L / total_L_L}')\n",
    "\n",
    "print(f'\\nType Annotation: {correct_TA_M + correct_TA_L}/{total_TA_M + total_TA_L} = {(correct_TA_M + correct_TA_L) / (total_TA_M + total_TA_L)}')\n",
    "print(f'No Type Annotation: {correct_L_M + correct_L_L}/{total_L_M + total_L_L} = {(correct_L_M + correct_L_L) / (total_L_M + total_L_L)}')\n",
    "print(f'Meaningful: {correct_TA_M + correct_L_M}/{total_TA_M + total_L_M} = {(correct_TA_M + correct_L_M) / (total_TA_M + total_L_M)}')\n",
    "print(f'Meaningless: {correct_TA_L + correct_L_L}/{total_TA_L + total_L_L} = {(correct_TA_L + correct_L_L) / (total_TA_L + total_L_L)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Answers per Snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_correctness_df: DataFrame = DataFrame(columns=['Task', 'Correctness', 'Meaningful', 'TypeAnnotation'])\n",
    "\n",
    "counter: int = 0\n",
    "\n",
    "for task in all_tasks:\n",
    "    correct_TA_M: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == True')['CorrectAnswer'].sum()\n",
    "    correct_TA_L: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == False')['CorrectAnswer'].sum()\n",
    "    correct_L_M: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == True')['CorrectAnswer'].sum()\n",
    "    correct_L_L: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == False')['CorrectAnswer'].sum()\n",
    "\n",
    "    total_TA_M: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == True')['CorrectAnswer'].count()\n",
    "    total_TA_L: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == False')['CorrectAnswer'].count()\n",
    "    total_L_M: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == True')['CorrectAnswer'].count()\n",
    "    total_L_L: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == False')['CorrectAnswer'].count()\n",
    "\n",
    "    print(f'\\nTask: {task}')\n",
    "    \n",
    "    print(f'\\nNo Type Annotation: {correct_L_M + correct_L_L}/{total_L_M + total_L_L} = {(correct_L_M + correct_L_L) / (total_L_M + total_L_L)}')\n",
    "    print(f'Type Annotation: {correct_TA_M + correct_TA_L}/{total_TA_M + total_TA_L} = {(correct_TA_M + correct_TA_L) / (total_TA_M + total_TA_L)}')\n",
    "\n",
    "    snippet_correctness_df.loc[counter] = [task, correct_TA_M / total_TA_M, True, True]\n",
    "    snippet_correctness_df.loc[counter + 1] = [task, correct_TA_L / total_TA_L, False, True]\n",
    "    snippet_correctness_df.loc[counter + 2] = [task, correct_L_M / total_L_M, True, False]\n",
    "    snippet_correctness_df.loc[counter + 3] = [task, correct_L_L / total_L_L, False, False]\n",
    "\n",
    "    counter += 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistic for the participants:\n",
    "- Number of Snippets\n",
    "- Number of Correct / Incorrect Snippets\n",
    "- Meaningful / Meaningless Snippets\n",
    "- Mean Time for Type Annotation / No Type Annotation\n",
    "- Overall Time Taken in Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the participants in the meaningless group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Meaningless Participants')\n",
    "for participant in meaningless_participants:\n",
    "    print(f'\\nParticipant {participant}')\n",
    "    number_of_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].count()\n",
    "    correct_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].sum()\n",
    "    print(f'Participant {participant} has {correct_snippets}/{number_of_snippets} correct snippets: {correct_snippets / number_of_snippets}')\n",
    "\n",
    "    mean_time: float = df_results_overall.query(f'ID == \"{participant}\"')['Time'].mean()\n",
    "    print(f'Participant {participant} has a mean time of {mean_time} seconds')\n",
    "\n",
    "    mean_time_TA: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['Time'].mean()\n",
    "    mean_time_L: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['Time'].mean()\n",
    "    print(f'Participant {participant} has a mean time of {mean_time_TA} seconds with Type Annotations and {mean_time_L} seconds without Type Annotations, thus the change is {mean_time_L - mean_time_TA} seconds')\n",
    "\n",
    "    overall_time: float = df_results_overall.query(f'ID == @participant')['Time'].sum()\n",
    "    print(f'Participant {participant} took {overall_time // 60} minutes and {overall_time % 60} seconds in total')\n",
    "\n",
    "print(f'\\n\\nMeaningful Participants')\n",
    "\n",
    "for participant in meaningful_participants:\n",
    "\n",
    "    print(f'\\nParticipant {participant}')\n",
    "    number_of_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].count()\n",
    "    correct_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].sum()\n",
    "    print(f'Participant {participant} has {correct_snippets}/{number_of_snippets} correct snippets: {correct_snippets / number_of_snippets}')\n",
    "\n",
    "    mean_time: float = df_results_overall.query(f'ID == \"{participant}\"')['Time'].mean()\n",
    "    print(f'Participant {participant} has a mean time of {mean_time} seconds')\n",
    "\n",
    "    mean_time_TA: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['Time'].mean()\n",
    "    mean_time_L: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['Time'].mean()\n",
    "    print(f'Participant {participant} has a mean time of {mean_time_TA} seconds with Type Annotations and {mean_time_L} seconds without Type Annotations, thus the change is {mean_time_L - mean_time_TA} seconds')\n",
    "\n",
    "    overall_time: float = df_results_overall.query(f'ID == @participant')['Time'].sum()\n",
    "    print(f'Participant {participant} took {overall_time // 60} minutes and {overall_time % 60} seconds in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the number of correct snippets as a ratio for participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_df_by_participant_meaningful: DataFrame = DataFrame(columns=['ID', 'Correctness', 'TypeAnnotation'])\n",
    "correctness_df_by_participant_meaningless: DataFrame = DataFrame(columns=['ID', 'Correctness', 'TypeAnnotation'])\n",
    "\n",
    "\n",
    "counter: int = 0\n",
    "for participant in meaningful_participants:\n",
    "    correct_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['CorrectAnswer'].sum()\n",
    "    correct_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['CorrectAnswer'].sum()\n",
    "\n",
    "    total_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['CorrectAnswer'].count()\n",
    "    total_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['CorrectAnswer'].count()\n",
    "\n",
    "    correctness_df_by_participant_meaningful.loc[counter] = [participant, correct_TA / total_TA, True]\n",
    "    correctness_df_by_participant_meaningful.loc[counter + 1] = [participant, correct_L / total_L, False]\n",
    "\n",
    "    counter += 2\n",
    "\n",
    "counter = 0\n",
    "for participant in meaningless_participants:\n",
    "    correct_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['CorrectAnswer'].sum()\n",
    "    correct_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['CorrectAnswer'].sum()\n",
    "\n",
    "    total_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['CorrectAnswer'].count()\n",
    "    total_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['CorrectAnswer'].count()\n",
    "\n",
    "    correctness_df_by_participant_meaningless.loc[counter] = [participant, correct_TA / total_TA, True]\n",
    "    correctness_df_by_participant_meaningless.loc[counter + 1] = [participant, correct_L / total_L, False]\n",
    "\n",
    "    counter += 2\n",
    "\n",
    "correctness_df_by_participant_meaningful.groupby(['TypeAnnotation', 'ID']).agg({'Correctness': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10), title=\"Meaningful\")\n",
    "\n",
    "correctness_df_by_participant_meaningless.groupby(['TypeAnnotation', 'ID']).agg({'Correctness': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10), title=\"Meaningless\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the time and correct answers per code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_correctness_df.groupby(['Meaningful', 'TypeAnnotation', 'Task']).agg({'Correctness': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty Rating\n",
    "\n",
    "First let's check how the difficulty is for each task. I think this would be best as a table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(df_results_overall.query(f'`Meaningful` == True'), x='Task', y='Difficulty', hue='TypeAnnotation', \n",
    "            gap=.1, \n",
    "            # inner=\"quart\", \n",
    "            # cut=0, \n",
    "            order=all_tasks,\n",
    "            # split=True,\n",
    "            )\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "plt.title('Difficulty per Task with Meaningful Identifier Names') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with meaningless identifier names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(df_results_overall.query(f'`Meaningful` == False'), x='Task', y='Difficulty',\n",
    "               hue='TypeAnnotation', \n",
    "            gap=.1, \n",
    "            inner=\"quart\", \n",
    "            cut=0, \n",
    "            order=all_tasks,\n",
    "            split=True,\n",
    "            )\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "plt.title('Difficulty per Task with and with Meaningless Identifier Names') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty that each participant felt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# first plot\n",
    "plot_L_M = axs[0].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == True')['Difficulty'] for participant in meaningful_participants], labels=meaningful_participants, positions=np.arange(len(meaningful_participants))*2.0+0.35, widths=0.6)\n",
    "plot_TA_M = axs[0].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == True')['Difficulty'] for participant in meaningful_participants], labels=meaningful_participants, positions=np.arange(len(meaningful_participants))*2.0-0.35, widths=0.6)\n",
    "\n",
    "# first plot settings\n",
    "axs[0].set_title('Mean Difficulty per Participant with Meaningful Identifier Names')\n",
    "axs[0].set_xticks(np.arange(0, len(meaningful_participants) * 2, 2), meaningful_participants, rotation=90)\n",
    "axs[0].set_yticks(np.arange(1, 6, 1))\n",
    "define_box_properties(plot_TA_M, 'blue', 'Type Annotation', axs[0])\n",
    "define_box_properties(plot_L_M, 'orange', 'No Type Annotation', axs[0])\n",
    "\n",
    "# second plot\n",
    "plot_L_L = axs[1].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == False')['Difficulty'] for participant in meaningless_participants], labels=meaningless_participants, positions=np.arange(len(meaningless_participants))*2.0+0.35, widths=0.6)\n",
    "plot_TA_L = axs[1].boxplot([df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == False')['Difficulty'] for participant in meaningless_participants], labels=meaningless_participants, positions=np.arange(len(meaningless_participants))*2.0-0.35, widths=0.6)\n",
    "\n",
    "# second plot settings\n",
    "axs[1].set_title('Mean Difficulty per Participant with Meaningless Identifier Names')\n",
    "axs[1].set_xticks(np.arange(0, len(meaningless_participants) * 2, 2), meaningless_participants, rotation=90)\n",
    "axs[1].set_yticks(np.arange(1, 6, 1))\n",
    "define_box_properties(plot_TA_L, 'blue', 'Type Annotation', axs[1])\n",
    "define_box_properties(plot_L_L, 'orange', 'No Type Annotation', axs[1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Answer by Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet_correctness_df.groupby(['Meaningful', 'TypeAnnotation']).agg({'Correctness': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulty count by snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "\n",
    "g = sns.FacetGrid(df_results_overall, row=\"Task\", \n",
    "                #   col=\"Type Annotation\",\n",
    "                  hue=\"Task\", \n",
    "                  height=1, \n",
    "                  aspect=30, \n",
    "                  palette=pal,\n",
    "                  )\n",
    "g.map_dataframe(sns.histplot, x=\"Difficulty\", hue=\"TypeAnnotation\", binwidth=1, binrange=(1, 6), multiple=\"stack\")\n",
    "# g.map_dataframe(sns.kdeplot, x=\"Difficulty\", clip=(1, 5), fill=True)\n",
    "# g.map_dataframe(sns.kdeplot, x=\"Difficulty\", hue='Type Annotation', clip_on=(1, 5), color=\"black\", lw=2, bw_adjust=.5)\n",
    "\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "g.map(label, \"Task\")\n",
    "g.figure.subplots_adjust(hspace=-.5)\n",
    "\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.set(xticks=np.arange(1, 6, 1))\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "g.figure.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display META Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the line 'Total' does not exist, create it\n",
    "if not 'Total' in df_meta_data.index:\n",
    "    df_meta_data.loc['Total'] = df_meta_data[[col for col in meta_data_columns if col not in unnecessary_columns_meta_data]].sum()\n",
    "    # add the number of times a participant did not finish all snippets to the 'Total' line\n",
    "    df_meta_data.loc['Total','NumberOfMissingSnippets'] = df_meta_data.groupby('ID')['NumberOfMissingSnippets'].any().value_counts()[True]\n",
    "    # df_meta_data.loc['Total']['NumberOfMissingSnippets'] = df_meta_data['NumberOfMissingSnippets'].any()\n",
    "    df_meta_data = df_meta_data.fillna('')\n",
    "\n",
    "# set the background color of the dataframe value to red if the value is False\n",
    "df_meta_data.style.applymap(lambda x: 'background-color: darkred' if x == False else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The corrected p-values for the mixed linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example p-values\n",
    "p_values = np.array([0.43, 0.033, 0.056, 0.031, 0.33, 0.1, 0.55, 0.97])\n",
    "\n",
    "# Perform Benjamini-Hochberg FDR correction\n",
    "alpha = 0.05\n",
    "rejected, pvals_corrected, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "\n",
    "# Output the results\n",
    "print(\"Original p-values:\", p_values)\n",
    "print(\"Corrected p-values:\", pvals_corrected)\n",
    "print(\"Rejected hypotheses:\", rejected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
