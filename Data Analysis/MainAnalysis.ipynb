{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from pandas import DataFrame\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.multitest import multipletests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Definitions and Constants\n",
    "\n",
    "Here I declare all necessary variables and constants. This includes the snippet names as well as the correct answers for each snippet. I further add the possible variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "snippet_answer: dict[str, str] = {\n",
    "    'arrayAverage': '4.0',\n",
    "    'binarySearch': '1',\n",
    "    'binaryToDecimal': '13',\n",
    "    'bubbleSort': '[1,2,3,4,5]',\n",
    "    'capitalizeFirstLetter': 'Hello World',\n",
    "    'commonChars': '2',\n",
    "    'containsSubstring': 'True',\n",
    "    'countIntegerInterval': '4',\n",
    "    'countLetters': '4',\n",
    "    'crossSum': '16',\n",
    "    'factorial': '24',\n",
    "    'forwardBackward': 'pricelesssselecirp',\n",
    "    'leastCommonMultiple': '30',\n",
    "    'linearSearch': '1',\n",
    "    'palindrome': 'True',\n",
    "    'power': '8',\n",
    "    'prime': 'True',\n",
    "    'squareRoot': '[3.0, 5.0, 4.0, 10.0]',\n",
    "    'unrolledSort': '[8, 9, 11, 12]',\n",
    "    'validParentheses': 'False',\n",
    "    'WarmUp': 'oefl',\n",
    "}\n",
    "\n",
    "possible_variations: dict[str, list[str]] = {\n",
    "    'group_meaningful': ['MT', 'MN'],\n",
    "    'group_obfuscated': ['LT', 'LN'],\n",
    "}\n",
    "\n",
    "# The following lists are used to create a dataframe for the General Information\n",
    "studentQuestions: list[str] = [\n",
    "    'StudyBefore',\n",
    "    'Job',\n",
    "    'CourseOfStudy',\n",
    "    'Semester',\n",
    "    'Algorithms',\n",
    "    'NrFalseInputs_studentQuestions',\n",
    "]\n",
    "progQuestions: list[str] = [\n",
    "    'YearsProgramming',\n",
    "    'ProgrammingLately',\n",
    "    'ProgrammingLanguages',\n",
    "    'RecentProgrammingLanguages',\n",
    "    'PythonProgramming',\n",
    "    'OverallExperience',\n",
    "    'Classmates',\n",
    "    'NrFalseInputs_progQuestions',\n",
    "]\n",
    "generalQuestions: list[str] = [\n",
    "    'Age',\n",
    "    'Gender',\n",
    "    'Eyesight',\n",
    "    'NrFalseInputs_generalQuestions',\n",
    "]\n",
    "miscellaneous_general_information: list[str] = [\n",
    "    'ActualScreenWidth',\n",
    "    'ActualScreenHeight',\n",
    "    'EyeXScreenWidth',\n",
    "    'EyeXScreenHeight',\n",
    "    'SubjectID',\n",
    "]\n",
    "\n",
    "# The following lists are used to create a dataframe for the Correctness and Time Data\n",
    "resultsOverall: list[str] = [\n",
    "    'Number',\n",
    "    'Task',\n",
    "    'Answer_Out',\n",
    "    'Time',\n",
    "    'TimeOut',\n",
    "    'SubjectID',\n",
    "    'CorrectAnswer',\n",
    "    'Meaningful',\n",
    "    'TypeAnnotation',\n",
    "]\n",
    "\n",
    "necessary_columns: list[str] = [\n",
    "    'ID',\n",
    "]\n",
    "\n",
    "meta_data_columns: list[str] = [\n",
    "    'GazeData',\n",
    "    'GazeDataFilled',\n",
    "    'PersonalInformation',\n",
    "    'ResultsOverall',\n",
    "    'PostQuestionnaire',\n",
    "    'DifficultyRating',\n",
    "    'InterviewData',\n",
    "    'OverallTime',\n",
    "    'Meaningful',\n",
    "    'Finished',\n",
    "    'NumberOfMissingSnippets',\n",
    "    'TrialData',\n",
    "]\n",
    "\n",
    "likert_mapping: dict[str, int] = {\n",
    "    'Very Inexperienced': 0,\n",
    "    'Inexperienced': 1,\n",
    "    'Neutral': 2,\n",
    "    'Experienced': 3,\n",
    "    'Very Experienced': 4,\n",
    "}\n",
    "\n",
    "figure_path = './figures/'\n",
    "\n",
    "difficulty_type = CategoricalDtype(categories=[1, 2, 3, 4, 5], ordered=True)\n",
    "experience_type = CategoricalDtype(\n",
    "    categories=['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'], ordered=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Definitions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "participants: list[str] = []  # list of participant numbers\n",
    "meaningful_participants: list[str] = []  # list of participant numbers who were assigned to the meaningful group\n",
    "obfuscated_participants: list[str] = []  # list of participant numbers who were assigned to the obfuscated group\n",
    "df_personal_information: DataFrame = pd.DataFrame(\n",
    "    columns=necessary_columns + generalQuestions + studentQuestions + progQuestions + miscellaneous_general_information)\n",
    "df_meta_data: DataFrame = pd.DataFrame(columns=necessary_columns + meta_data_columns)\n",
    "df_difficulty_rating: DataFrame = pd.DataFrame(columns=['ID', 'Task', 'Difficulty', 'Comment'])\n",
    "df_subjective_feelings: DataFrame = pd.DataFrame()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generally helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def define_box_properties(plot_name, color_code, label, axis=None):\n",
    "    for k, v in plot_name.items():\n",
    "        plt.setp(plot_name.get(k), color=color_code)\n",
    "\n",
    "    # use plot function to draw a small line to name the legend\n",
    "    if axis is not None:\n",
    "        axis.plot([], c=color_code, label=label)\n",
    "        axis.legend()\n",
    "    else:\n",
    "        plt.plot([], c=color_code, label=label)\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            tmp = df[df[r].notnull() & df[c].notnull()]\n",
    "            pvalues[r][c] = round(stats.pearsonr(tmp[r], tmp[c])[1], 4)\n",
    "    return pvalues\n",
    "\n",
    "\n",
    "def save_trial_start_times(file_path):\n",
    "    # Load with semicolon separator\n",
    "    df = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "    # Remove fixation cross entries\n",
    "    stimulus_df = df[~df[\"Snippet\"].str.contains(\"FixationCross\")]\n",
    "\n",
    "    # Add SnippetPath column similar to Snippet but with .jpg ending\n",
    "    stimulus_df[\"SnippetPath\"] = stimulus_df[\"Snippet\"] + \".jpg\"\n",
    "\n",
    "    # Get first entry for each trial\n",
    "    first_times = (\n",
    "        stimulus_df.groupby(\"TrialNumber\")\n",
    "        .first()\n",
    "        .reset_index()[[\"TrialNumber\", \"Snippet\", \"Time\", \"SnippetPath\"]]\n",
    "    )\n",
    "\n",
    "    # Auto-number sequence\n",
    "    first_times.insert(0, \"Sequence\", range(len(first_times)))\n",
    "\n",
    "    # Clean StartingTime: replace comma with dot, then convert to int\n",
    "    first_times[\"Time\"] = (\n",
    "        first_times[\"Time\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "        .astype(float)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    first_times = first_times.rename(columns={\"TrialNumber\": \"ID\", \"Time\": \"StartingTime\"})\n",
    "    if len(first_times) == 0:\n",
    "        print(len(first_times), \"start times found in\", file_path)\n",
    "\n",
    "    # Save to CSV\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    parent_dir = os.path.dirname(file_path)\n",
    "    output_path = os.path.join(parent_dir, f\"{base_name}_start_times.csv\")\n",
    "    first_times.to_csv(output_path, index=False, sep=\";\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all data\n",
    "\n",
    "This includes:\n",
    "- [x] ~~Gaze Data~~\n",
    "- [x] ~~Individual Summary~~\n",
    "- [x] ~~Personal Information~~\n",
    "- [x] ~~Correctness and Time~~\n",
    "- [x] ~~Interview Data~~\n",
    "\n",
    "First we need to find all possible folders and files. They are located in `Participants`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Participants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "participants: list[str] = sorted(os.walk('./data').__next__()[1])\n",
    "\n",
    "df_meta_data['ID'] = participants"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for all files\n",
    "\n",
    "Collect all files that are present and collect which are still missing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#  please check if file '`ID` PostQuestionaire.pdf' exists\n",
    "for participant in participants:\n",
    "    # check if '`ID` Post-Questionnaire.pdf' exists\n",
    "    if not os.path.isfile(f'./data/{participant}/{participant} Post-Questionnaire.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PostQuestionnaire'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PostQuestionnaire'] = True\n",
    "\n",
    "    # check if '`ID`' exists\n",
    "    if not os.path.isfile(f'./data/{participant}/DifficultyRating_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'DifficultyRating'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'DifficultyRating'] = True\n",
    "\n",
    "    # check if the folder 'Trial_`ID`' exists\n",
    "    if not os.path.isdir(f'./data/{participant}/Trial_{participant}'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'TrialData'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'TrialData'] = True\n",
    "\n",
    "    # check if 'GeneralInfo_`ID`.csv' exists\n",
    "    if not os.path.isfile(f'./data/{participant}/Trial_{participant}/GeneralInfo_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PersonalInformation'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'PersonalInformation'] = True\n",
    "\n",
    "    # check if 'ResultsOverall_`ID`.csv' exists\n",
    "    if not os.path.isfile(f'./data/{participant}/Trial_{participant}/ResultsOverall_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'ResultsOverall'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'ResultsOverall'] = True\n",
    "\n",
    "    # check if 'GazeData_`ID`.csv' exists\n",
    "    if not os.path.isfile(f'./data/{participant}/Trial_{participant}/GazeData_{participant}.csv'):\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'GazeData'] = False\n",
    "    else:\n",
    "        df_meta_data.loc[df_meta_data['ID'] == participant, 'GazeData'] = True\n",
    "        save_trial_start_times(f'./data/{participant}/Trial_{participant}/GazeData_{participant}.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all Meta Information about Files concerning the study itself."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print the count of False GazeData vs the count of True GazeData and show which ones are currently missing\n",
    "gaze_data_count = df_meta_data['GazeData'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('GazeDataFiles found:', gaze_data_count[True], len(df_meta_data['GazeData'])))\n",
    "\n",
    "# print the count of False PersonalInformation vs the count of True PersonalInformation and show which ones are currently missing\n",
    "personal_information_count = df_meta_data['PersonalInformation'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('PersonalInformationFiles found:', personal_information_count[True],\n",
    "                                 len(df_meta_data['PersonalInformation'])))\n",
    "\n",
    "# print the count of False ResultsOverall vs the count of True ResultsOverall and show which ones are currently missing\n",
    "results_overall_count = df_meta_data['ResultsOverall'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('ResultsOverallFiles found:', results_overall_count[True],\n",
    "                                 len(df_meta_data['ResultsOverall'])))\n",
    "\n",
    "# print the count of False PostQuestionnaire vs the count of True PostQuestionnaire and show which ones are currently missing\n",
    "post_questionnaire_count = df_meta_data['PostQuestionnaire'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('PostQuestionnaireFiles found:', post_questionnaire_count[True],\n",
    "                                 len(df_meta_data['PostQuestionnaire'])))\n",
    "\n",
    "# print the count of False DifficultyRating vs the count of True DifficultyRating and show which ones are currently missing\n",
    "difficulty_rating_count = df_meta_data['DifficultyRating'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('DifficultyRatingFiles found:', difficulty_rating_count[True],\n",
    "                                 len(df_meta_data['DifficultyRating'])))\n",
    "\n",
    "# print the count of False TrialData vs the count of True TrialData and show which ones are currently missing\n",
    "trial_data_count = df_meta_data['TrialData'].value_counts()\n",
    "print('{:<32} {:2}/{:>2}'.format('TrialDataFolders found:', trial_data_count[True], len(df_meta_data['TrialData'])))\n",
    "\n",
    "# print which ones are missing\n",
    "print('{:56} {}'.format('The following participants have no GazeData:',\n",
    "                        list(df_meta_data.loc[df_meta_data['GazeData'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no PersonalInformation:',\n",
    "                        list(df_meta_data.loc[df_meta_data['PersonalInformation'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no ResultsOverall:',\n",
    "                        list(df_meta_data.loc[df_meta_data['ResultsOverall'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no PostQuestionnaire:',\n",
    "                        list(df_meta_data.loc[df_meta_data['PostQuestionnaire'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no DifficultyRating:',\n",
    "                        list(df_meta_data.loc[df_meta_data['DifficultyRating'] == False, 'ID'].values)))\n",
    "print('{:56} {}'.format('The following participants have no TrialData:',\n",
    "                        list(df_meta_data.loc[df_meta_data['TrialData'] == False, 'ID'].values)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the `df_meta_data` DataFrame of columns that are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unnecessary_columns_meta_data: list[str] = [\n",
    "    'PersonalInformation',\n",
    "    'ResultsOverall',\n",
    "    'PostQuestionnaire',\n",
    "    'DifficultyRating',\n",
    "    'TrialData',\n",
    "]\n",
    "\n",
    "# remove unnecessary columns from df_meta_data\n",
    "try:\n",
    "    df_meta_data.drop(columns=unnecessary_columns_meta_data, inplace=True)\n",
    "\n",
    "except:\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Information\n",
    "\n",
    "I read in all data from the files containing the personal information (`GeneralInfo_{ID}`) and add the `ID` for the participant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(0, len(participants)):\n",
    "    try:\n",
    "        df_personal_information.loc[i] = \\\n",
    "            pd.read_csv(\n",
    "                f'./data/{participants[i]}/Trial_{participants[i]}/GeneralInfo_{participants[i]}.csv',\n",
    "                sep=';').iloc[0]\n",
    "        df_personal_information.loc[i, 'ID'] = participants[i]\n",
    "        df_meta_data.loc[i, 'PersonalInformation'] = True\n",
    "\n",
    "    except:\n",
    "        print(f'Participant {participants[i]} has no GeneralInfo.csv file')\n",
    "        df_meta_data.loc[i, 'PersonalInformation'] = False\n",
    "\n",
    "# write personal_information into a csv\n",
    "df_personal_information.to_csv(f'./studies/Linearity/GeneralInfo_AllParticipants.csv', sep=';', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty Rating\n",
    "\n",
    "Read in all the difficulty rating to merge it later on into the results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(0, len(participants)):\n",
    "    try:\n",
    "        df_to_add = pd.read_csv(f'./data/{participants[i]}/DifficultyRating_{participants[i]}.csv', sep=';')\n",
    "        if not all([x in snippet_answer.keys() for x in df_to_add[\"Task\"]]):\n",
    "            print(\n",
    "                f'Participant {participants[i]} has a problem in the DifficultyRating file; these names {[x for x in df_to_add[\"Task\"] if x not in snippet_answer.keys()]} are not in the snippet_answer dictionary')\n",
    "\n",
    "        df_to_add['ID'] = participants[i]\n",
    "        df_difficulty_rating = pd.concat([df_difficulty_rating, df_to_add], ignore_index=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_difficulty_rating['Difficulty'] = df_difficulty_rating['Difficulty'].astype(difficulty_type)\n",
    "\n",
    "# df_difficulty_rating"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjective Feelings\n",
    "\n",
    "Read in all subjective feelings to merge it later on into the personal information DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for i in range(0, len(participants)):\n",
    "    try:\n",
    "        df_to_add = pd.read_csv(f'./data/{participants[i]}/{participants[i]} Post-Questionnaire.csv',\n",
    "                                sep=';')\n",
    "        df_to_add['ID'] = participants[i]\n",
    "        df_subjective_feelings = pd.concat([df_subjective_feelings, df_to_add], ignore_index=True)\n",
    "    except:\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning for Personal Information\n",
    "\n",
    "We can drop all columns that contain the number of false Inputs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unnecessary_columns_gen_info: list[str] = [\n",
    "    'NrFalseInputs_studentQuestions',\n",
    "    'NrFalseInputs_progQuestions',\n",
    "    'NrFalseInputs_generalQuestions',\n",
    "]\n",
    "\n",
    "# remove unnecessary columns\n",
    "try:\n",
    "    df_personal_information.drop(columns=unnecessary_columns_gen_info, inplace=True)\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# remove participants that are not currently in their bachelor but also mention them\n",
    "#print(df_personal_information.query('Job != \"Undergraduate Student (Bachelor Studies)\"')['ID'])\n",
    "#df_personal_information.drop(df_personal_information[df_personal_information['Job'] != 'Undergraduate Student (Bachelor Studies)'].index, inplace=True)\n",
    "\n",
    "df_personal_information['PythonProgramming'] = df_personal_information['PythonProgramming'].astype(experience_type)\n",
    "df_personal_information['OverallExperience'] = df_personal_information['OverallExperience'].astype(experience_type)\n",
    "df_personal_information['Classmates'] = df_personal_information['Classmates'].astype(experience_type)\n",
    "\n",
    "category_mapping = {'Very Inexperienced': 1, 'Inexperienced': 2, 'Average': 3, 'Experienced': 4, 'Very Experienced': 5}\n",
    "df_personal_information['Classmates'] = df_personal_information['Classmates'].map(category_mapping)\n",
    "df_personal_information['PythonProgramming'] = df_personal_information['PythonProgramming'].map(category_mapping)\n",
    "df_personal_information['OverallExperience'] = df_personal_information['OverallExperience'].map(category_mapping)\n",
    "\n",
    "df_personal_information"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subjective Difficulty Merging\n",
    "\n",
    "Merging the subjective feelings into the personal information "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# merge personal information and the subjective feelings\n",
    "df_personal_information = pd.merge(df_personal_information, df_subjective_feelings, how='left', left_on=['ID'],\n",
    "                                   right_on=['ID'])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of the personal information\n",
    "\n",
    "1. Correct and throw out any person that is not in CS-related courses of study\n",
    "2. Throw out anyone under the age of 18"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "course_of_study_mapping: dict[str, str] = {\n",
    "    'cs': 'Computer Science',\n",
    "    'CS': 'Computer Science',\n",
    "    'BCs. Informatik': 'Computer Science',\n",
    "    'Cybersicherheit': 'Cybersecurity',\n",
    "    'DSAI': 'Data Science and Artificial Intelligence',\n",
    "    'B. Sc. Business Informatics': 'Business Informatics',\n",
    "    'Informatik B.Sc': 'Computer Science',\n",
    "    'Eingebettete Systeme': 'Embedded Systems',\n",
    "    'Informatik': 'Computer Science',\n",
    "    'Math and Computer Science': 'Mathematics and Computer Science',\n",
    "    'Computer Science (German)': 'Computer Science',\n",
    "    'Mathematik und Informatik': 'Mathematics and Computer Science',\n",
    "    'Medieninformatik': 'Media Informatics',\n",
    "    'medieninfo': 'Media Informatics',\n",
    "    'Informatics': 'Computer Science',\n",
    "    'Informatik Kernbereich': 'Computer Science',\n",
    "    'Eingebettete Systeme B.Sc.': 'Embedded Systems',\n",
    "    'Computerlinguistik B.Sc.': 'Computational Linguistics',\n",
    "    'materialise science': 'Material Science'\n",
    "}\n",
    "\n",
    "possible_courses_of_study: list[str] = ['Computer Science',\n",
    "                                        'Cybersecurity',\n",
    "                                        'Business Informatics',\n",
    "                                        'Data Science and Artificial Intelligence',\n",
    "                                        'Mathematics and Computer Science',\n",
    "                                        'Media Informatics',\n",
    "                                        'Embedded Systems',\n",
    "                                        'Bioinformatics',\n",
    "                                        'Computational Linguistics',\n",
    "                                        'Material Science',\n",
    "                                        'Bachelors Plus MINT',\n",
    "                                        'Systems Engineering'\n",
    "                                        ]\n",
    "\n",
    "# check the age and throw out anyone under the age of 18\n",
    "print(f'Have to drop {df_personal_information.query(\"Age < 18\")[\"ID\"].tolist()} because they were under the age of 18')\n",
    "df_personal_information.drop(df_personal_information[df_personal_information['Age'] < 18].index, inplace=True)\n",
    "\n",
    "# normalize all the course of study values\n",
    "df_personal_information['CourseOfStudy'] = df_personal_information['CourseOfStudy'].replace(course_of_study_mapping)\n",
    "mask = ~df_personal_information['CourseOfStudy'].isin(possible_courses_of_study)\n",
    "print(\n",
    "    f\"Have to drop {df_personal_information.loc[mask, 'CourseOfStudy'].tolist()} because they were not in the course of study mapping\")\n",
    "print(df_personal_information.loc[mask, 'ID'])\n",
    "\n",
    "df_personal_information = df_personal_information.loc[~mask].reset_index(drop=True)\n",
    "\n",
    "# check and throw out anyone that is not studying computer science related fields\n",
    "non_cs_students: list[str] = df_personal_information.query(\"CourseOfStudy != @possible_courses_of_study\")[\n",
    "    \"CourseOfStudy\"].tolist()\n",
    "print(f'Have to drop {len(non_cs_students)} {non_cs_students} because they were not studying Computer Science')\n",
    "\n",
    "# change columns to numeric value\n",
    "df_personal_information['Age'] = pd.to_numeric(df_personal_information['Age'])\n",
    "df_personal_information['Semester'] = pd.to_numeric(df_personal_information['Semester'])\n",
    "df_personal_information['YearsProgramming'] = pd.to_numeric(df_personal_information['YearsProgramming'])\n",
    "df_personal_information['Eyesight'] = df_personal_information['Eyesight'].astype(\"category\")\n",
    "df_personal_information['Gender'] = df_personal_information['Gender'].astype(\"category\")\n",
    "\n",
    "# remaining participant IDs\n",
    "participants = df_personal_information['ID'].unique().tolist()\n",
    "print(f'Remaining participants: {len(participants)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness and Time\n",
    "\n",
    "I read in all data from the files containing the results (`ResultsOverall_{ID}`) and add the `ID` for the participant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall: DataFrame = pd.DataFrame(columns=necessary_columns + resultsOverall)\n",
    "\n",
    "for i in range(0, len(participants)):\n",
    "\n",
    "    df_to_add_general: DataFrame = DataFrame()\n",
    "\n",
    "    # check if the participant has finished all snippets and read in the results data\n",
    "    try:\n",
    "        df_to_add: DataFrame = pd.read_csv(\n",
    "            f'./data/{participants[i]}/Trial_{participants[i]}/ResultsOverall_{participants[i]}.csv',\n",
    "            sep=';').assign(ID=participants[i])\n",
    "\n",
    "        if df_to_add['Answer_Out'].isnull().values.any():\n",
    "            df_meta_data.loc[i, 'Finished'] = False\n",
    "        else:\n",
    "            df_meta_data.loc[i, 'Finished'] = True\n",
    "\n",
    "        df_to_add['ID'] = participants[i]\n",
    "        df_results_overall = pd.concat([df_results_overall, df_to_add], ignore_index=True)\n",
    "\n",
    "        df_meta_data.loc[i, 'ResultsOverall'] = True\n",
    "\n",
    "    except:\n",
    "        print(f'Participant {participants[i]} has no ResultsOverall.csv file')\n",
    "        df_meta_data.loc[i, 'ResultsOverall'] = False\n",
    "\n",
    "# now get the values from the eyetracking and merge it into the dataframe\n",
    "# TODO: this file does not yet exist, create it first\n",
    "df_eye_tracking_metrics: DataFrame = pd.read_csv(f'./output/AOI/Metrics_Data_for_Anova.csv', sep=';')\n",
    "df_results_overall['SubjectID'] = df_results_overall['SubjectID'].apply(lambda x: x[:8])\n",
    "df_results_overall = pd.merge(df_results_overall, df_eye_tracking_metrics, how='left', left_on=['SubjectID', 'Task'],\n",
    "                              right_on=['Participant', 'Snippet'])\n",
    "\n",
    "# sort the dataframe by the participant `ID` and `Number`\n",
    "df_results_overall.sort_values(by=['ID', 'Number'], inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the meaningful tag to the personal information dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning for Correctness and Time\n",
    "\n",
    "We can drop the column `SubjectID` and turn the `Time` column into seconds."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "unnecessary_columns_overall_results: list[str] = [\n",
    "    'SubjectID',\n",
    "    'Number',\n",
    "    'Participant',\n",
    "    'Expert',\n",
    "    'ExecOrder_Naive_Score',\n",
    "    'ExecOrder_Dynamic_Score',\n",
    "    'ExecOrder_Dynamic_Repetitions',\n",
    "    'Snippet',\n",
    "]\n",
    "\n",
    "# remove unnecessary columns\n",
    "try:\n",
    "    df_results_overall.drop(columns=unnecessary_columns_overall_results, inplace=True, ignore_index=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Show whether a participant did all snippets or if they ran into a timeout\n",
    "for i in range(0, len(participants)):\n",
    "    if df_results_overall.query(f'ID == \"{participants[i]}\"')['Answer_Out'].isnull().values.any():\n",
    "        number_of_missing_snippets: int = df_results_overall.query(f'ID == \"{participants[i]}\"')[\n",
    "            \"Answer_Out\"].isnull().sum()\n",
    "        print(\n",
    "            f'Participant {participants[i]} did not finish all snippets, they missed {number_of_missing_snippets} snippets.')\n",
    "        df_meta_data.loc[i, 'NumberOfMissingSnippets'] = number_of_missing_snippets\n",
    "    else:\n",
    "        df_meta_data.loc[i, 'NumberOfMissingSnippets'] = 0\n",
    "\n",
    "    if df_results_overall.query(f'ID == \"{participants[i]}\"')['TimeOut'].any():\n",
    "        print(f'Participant {participants[i]} ran into a timeout.')\n",
    "\n",
    "# drop any line that has a `Time` value of 0\n",
    "df_results_overall: DataFrame = df_results_overall[df_results_overall['Time'] != 0]\n",
    "\n",
    "# drop every line that has a `Task` value of `WarmUp`\n",
    "df_results_overall: DataFrame = df_results_overall[df_results_overall['Task'] != 'WarmUp']\n",
    "\n",
    "# turn the `Time` column into seconds as it is currently in milliseconds\n",
    "df_results_overall['Time'] = df_results_overall['Time'].apply(lambda x: x // 1000 if x > 1000 else x)\n",
    "\n",
    "# fill the columns `Meaningful` and `Type Annotation` with the correct values\n",
    "df_results_overall['Meaningful'] = df_results_overall['Task'].apply(lambda x: True if x[-2] == 'M' else False)\n",
    "df_results_overall['TypeAnnotation'] = df_results_overall['Task'].apply(lambda x: True if x[-1] == 'T' else False)\n",
    "\n",
    "# set the column of Meaningful to True if the participant has at least one meaningful snippet\n",
    "df_meta_data['Meaningful'] = df_meta_data.apply(\n",
    "    lambda x: True if df_results_overall.query(f'ID == \"{x[\"ID\"]}\"')['Meaningful'].any() else False, axis=1)\n",
    "\n",
    "# save which of the participants were assigned to the respective groups\n",
    "meaningful_participants = df_meta_data.query('Meaningful == True')['ID'].values\n",
    "obfuscated_participants = df_meta_data.query('Meaningful == False')['ID'].values\n",
    "print(f'Number of meaningful runs: {len(meaningful_participants)}')\n",
    "print(\n",
    "    f'Number of meaningful runs with eye tracking data: {df_meta_data.query(\"Meaningful == True & GazeData == True\")[\"ID\"].nunique()}')\n",
    "print(f'Number of obfuscated runs: {len(obfuscated_participants)}')\n",
    "print(\n",
    "    f'Number of obfuscated runs with eye tracking data: {df_meta_data.query(\"Meaningful == False & GazeData == True\")[\"ID\"].nunique()}')\n",
    "# remove the last two letters from the `Task` column\n",
    "df_results_overall['Task'] = df_results_overall['Task'].apply(lambda x: x[:-2] if x[-2] == 'M' or x[-2] == 'L' else x)\n",
    "\n",
    "# compute if the correct answer was given for each `Task` and `ID` and add it to `CorrectAnswer`\n",
    "df_results_overall['CorrectAnswer'] = df_results_overall.apply(\n",
    "    lambda x: True if snippet_answer[x['Task']] == x['Answer_Out'] else False, axis=1)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge `df_results_overall` and `df_difficulty_rating` and also merge `personal_information` and the `Meaningful` columns of `df_results_overall`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall = pd.merge(df_results_overall, df_difficulty_rating, how='left', left_on=['ID', 'Task'],\n",
    "                              right_on=['ID', 'Task'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computations for Correctness and Time\n",
    "\n",
    "First, we create a general overview of the tables data.\n",
    "\n",
    "We compute the time taken for each `Task` for each participant `ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the meaningful and obfuscated values to the personal information DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_personal_information['Meaningful'] = df_personal_information['ID'].apply(\n",
    "    lambda x: True if x in meaningful_participants else False)\n",
    "\n",
    "df_personal_information"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall = pd.merge(df_results_overall, df_personal_information, how='left', left_on=['ID', 'Meaningful'],\n",
    "                              right_on=['ID', 'Meaningful'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall.groupby(['ID', 'Task']).agg({'Time': 'mean'}).unstack().T.fillna('-')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Aggregate one row per participant (take 'first' for demographic fields) ---\n",
    "df_participants = (\n",
    "    df_results_overall.groupby('ID')\n",
    "    .agg({\n",
    "        'Meaningful': 'first',\n",
    "        'Gender': 'first',\n",
    "        'Age': 'first',\n",
    "        'Semester': 'first',\n",
    "        'YearsProgramming': 'first'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute Ns\n",
    "n_total = len(df_participants)\n",
    "n_meaningful = df_participants['Meaningful'].sum() if df_participants['Meaningful'].dtype == bool else \\\n",
    "    df_participants[df_participants['Meaningful'] == True].shape[0]\n",
    "n_not_meaningful = n_total - n_meaningful\n",
    "\n",
    "# --- Gender crosstab & percentages (row-wise within Meaningful groups) ---\n",
    "gender_counts = pd.crosstab(df_participants['Gender'], df_participants['Meaningful'])\n",
    "# compute percentages within each Meaningful column\n",
    "gender_percentages = gender_counts.div(gender_counts.sum(axis=0), axis=1) * 100\n",
    "\n",
    "# --- Print gender table (clean text) ---\n",
    "print(\"\\nGender distribution (counts and % within each Meaningful group):\")\n",
    "header = f\"{'Gender':<22} {'Meaningful=False (N=' + str(n_not_meaningful) + ')':<28} {'Meaningful=True (N=' + str(n_meaningful) + ')':<28}\"\n",
    "print(header)\n",
    "print('-' * len(header))\n",
    "\n",
    "for gender in gender_counts.index:\n",
    "    count_false = int(gender_counts.loc[gender, False]) if False in gender_counts.columns and not pd.isna(\n",
    "        gender_counts.loc[gender, False]) else 0\n",
    "    count_true = int(gender_counts.loc[gender, True]) if True in gender_counts.columns and not pd.isna(\n",
    "        gender_counts.loc[gender, True]) else 0\n",
    "\n",
    "    pct_false = gender_percentages.loc[gender, False] if False in gender_percentages.columns else 0\n",
    "    pct_true = gender_percentages.loc[gender, True] if True in gender_percentages.columns else 0\n",
    "\n",
    "    val_false = f\"{count_false} ({pct_false:.0f}%)\" if count_false > 0 else \"---\"\n",
    "    val_true = f\"{count_true} ({pct_true:.0f}%)\" if count_true > 0 else \"---\"\n",
    "\n",
    "    print(f\"{gender:<22} {val_false:<28} {val_true:<28}\")\n",
    "\n",
    "\n",
    "# --- Function to compute mean ± sd and overall ---\n",
    "def mean_sd_text(series):\n",
    "    s = series.dropna()\n",
    "    if len(s) == 0:\n",
    "        return (\"---\", \"---\")\n",
    "    return (f\"{s.mean():.2f}\", f\"{s.std(ddof=1):.2f}\")\n",
    "\n",
    "\n",
    "# --- Continuous variables summary with overall ---\n",
    "cont_vars = [\n",
    "    ('Age', 'Age (in Years)'),\n",
    "    ('Semester', 'Semester'),\n",
    "    ('YearsProgramming', 'Years of Programming')\n",
    "]\n",
    "\n",
    "print(\"\\nContinuous variables (mean ± SD):\")\n",
    "header2 = f\"{'Variable':<25} {'Meaningful=False':<20} {'Meaningful=True':<20} {'Overall':<20}\"\n",
    "print(header2)\n",
    "print('-' * (len(header2) + 10))\n",
    "\n",
    "for col, label in cont_vars:\n",
    "    # per-group\n",
    "    grp = df_participants.groupby('Meaningful')[col]\n",
    "    mean_false, sd_false = (\"---\", \"---\")\n",
    "    mean_true, sd_true = (\"---\", \"---\")\n",
    "\n",
    "    if False in grp.groups:\n",
    "        mean_false, sd_false = mean_sd_text(grp.get_group(False))\n",
    "    if True in grp.groups:\n",
    "        mean_true, sd_true = mean_sd_text(grp.get_group(True))\n",
    "\n",
    "    # overall\n",
    "    mean_overall, sd_overall = mean_sd_text(df_participants[col])\n",
    "\n",
    "    text_false = f\"{mean_false} ± {sd_false}\" if mean_false != \"---\" else \"---\"\n",
    "    text_true = f\"{mean_true} ± {sd_true}\" if mean_true != \"---\" else \"---\"\n",
    "    text_over = f\"{mean_overall} ± {sd_overall}\" if mean_overall != \"---\" else \"---\"\n",
    "\n",
    "    print(f\"{label:<25} {text_false:<20} {text_true:<20} {text_over:<20}\")\n",
    "\n",
    "# --- Also print sample sizes explicitly for clarity ---\n",
    "print(\n",
    "    f\"\\nSample sizes: Total N = {n_total}, Meaningful=True N = {n_meaningful}, Meaningful=False N = {n_not_meaningful}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse Correctness and Time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "aggregation_computation: dict[str, str] = {\n",
    "    'CorrectAnswer': [np.count_nonzero, ],\n",
    "    'TypeAnnotation': ['count', ],\n",
    "    'Time': ['mean', 'median', 'min', 'max', 'sum', ],\n",
    "}\n",
    "\n",
    "overview_correctness_time: DataFrame = df_results_overall.groupby(['ID']).agg(aggregation_computation).reset_index()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
    "fig.suptitle('Distribution of Time and Correctness', fontsize=16)\n",
    "\n",
    "# Plot 1: Time Distribution\n",
    "time_std_threshold = 3\n",
    "time_col = ('Time', 'mean')\n",
    "mean_time = overview_correctness_time[time_col].mean()\n",
    "std_time = overview_correctness_time[time_col].std()\n",
    "upper_bound_time = mean_time + time_std_threshold * std_time\n",
    "lower_bound_time = mean_time - time_std_threshold * std_time\n",
    "outliers_time = overview_correctness_time[\n",
    "    (overview_correctness_time[time_col] > upper_bound_time) | (overview_correctness_time[time_col] < lower_bound_time)]\n",
    "\n",
    "sns.histplot(overview_correctness_time[time_col], kde=True, ax=axs[0], label='Time Distribution')\n",
    "axs[0].set_title(f'Total Time per Participant (Outliers > {time_std_threshold}σ)')\n",
    "axs[0].set_xlabel('Total Time (seconds)')\n",
    "axs[0].set_ylabel('Number of Participants')\n",
    "axs[0].axvline(mean_time, color='red', linestyle='--', label=f'Mean: {mean_time:.2f}s')\n",
    "axs[0].axvline(upper_bound_time, color='orange', linestyle=':',\n",
    "               label=f'Upper Bound (+{time_std_threshold}σ): {upper_bound_time:.2f}s')\n",
    "if lower_bound_time > 0:\n",
    "    axs[0].axvline(lower_bound_time, color='orange', linestyle=':',\n",
    "                   label=f'Lower Bound (-{time_std_threshold}σ): {lower_bound_time:.2f}s')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot 2: Correct Answer Distribution\n",
    "correct_col = ('CorrectAnswer', 'count_nonzero')\n",
    "mean_correct = overview_correctness_time[correct_col].mean()\n",
    "std_correct = overview_correctness_time[correct_col].std()\n",
    "lower_bound_correct = 11\n",
    "\n",
    "# Create a boolean Series for filtering\n",
    "is_outlier_correct = (overview_correctness_time[correct_col] < lower_bound_correct)\n",
    "outliers_correct = overview_correctness_time[is_outlier_correct]\n",
    "\n",
    "sns.histplot(overview_correctness_time[correct_col], kde=True, ax=axs[1], color='green', binwidth=1)\n",
    "axs[1].set_title(f'Correct Answers per Participant')\n",
    "axs[1].set_xlabel('Number of Correct Answers')\n",
    "axs[1].set_ylabel('Number of Participants')\n",
    "axs[1].axvline(mean_correct, color='red', linestyle='--', label=f'Mean: {mean_correct:.2f}')\n",
    "axs[1].axvline(lower_bound_correct, color='orange', linestyle=':', label=f'Lower Bound (10/20)')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\"Time analysis: {len(outliers_time)} participant(s) found as outliers outside of {time_std_threshold} standard deviations from the mean ({mean_time:.2f}s).\")\n",
    "if not outliers_time.empty:\n",
    "    print(\"Outlier participant(s) by ID and their total time:\")\n",
    "    print(pd.concat([outliers_time['ID'], outliers_time[time_col]], axis=1))\n",
    "\n",
    "print(\n",
    "    f\"\\nCorrectness analysis: {len(outliers_correct)} participant(s) found as outliers below {lower_bound_correct}/20).\")\n",
    "if not outliers_correct.empty:\n",
    "    print(\"Outlier participant(s) by ID and their counts of correct answers:\")\n",
    "    print(pd.concat([outliers_correct['ID'], outliers_correct[correct_col]], axis=1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove participants 12663, 13020, 24280, 29206, 31195 because correctness below 11"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "outlier_participants: list[str] = ['12663', '13020', '24280', '29206', '31195']\n",
    "overview_correctness_time = overview_correctness_time[~overview_correctness_time['ID'].isin(outlier_participants)]\n",
    "\n",
    "df_results_overall = df_results_overall[~df_results_overall['ID'].isin(outlier_participants)]\n",
    "df_personal_information = df_personal_information[~df_personal_information['ID'].isin(outlier_participants)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
    "fig.suptitle('Distribution of Time and Correctness (After Removing Outliers)', fontsize=16)\n",
    "# Plot 1: Time Distribution\n",
    "time_col = ('Time', 'mean')\n",
    "mean_time = overview_correctness_time[time_col].mean()\n",
    "\n",
    "sns.histplot(overview_correctness_time[time_col], kde=True, ax=axs[0], label='Time Distribution')\n",
    "axs[0].set_title(f'Total Time per Participant (After Removing Outliers)')\n",
    "axs[0].set_xlabel('Total Time (seconds)')\n",
    "axs[0].set_ylabel('Number of Participants')\n",
    "axs[0].axvline(mean_time, color='red', linestyle='--', label=f'Mean: {mean_time:.2f}s')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot 2: Correct Answer Distribution\n",
    "correct_col = ('CorrectAnswer', 'count_nonzero')\n",
    "mean_correct = overview_correctness_time[correct_col].mean()\n",
    "\n",
    "sns.histplot(overview_correctness_time[correct_col], kde=True, ax=axs[1], color='green', binwidth=1)\n",
    "axs[1].set_title(f'Correct Answers per Participant (After Removing Outliers)')\n",
    "axs[1].set_xlabel('Number of Correct Answers')\n",
    "axs[1].set_ylabel('Number of Participants')\n",
    "axs[1].axvline(mean_correct, color='red', linestyle='--', label=f'Mean: {mean_correct:.2f}')\n",
    "axs[1].legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove certain data points"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Drop rows with any NaN in Time, NumberOfFixations\n",
    "df_results_overall = df_results_overall.dropna(\n",
    "    subset=[\"Time\", \"NumberOfFixations\"]\n",
    ").copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Add new column for true fixations per second\n",
    "df_results_overall[\"TrueFixationsPerSecond\"] = (\n",
    "        df_results_overall[\"NumberOfFixations\"] / df_results_overall[\"Time\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Max time (in seconds)\n",
    "MAX_TIME = 300\n",
    "\n",
    "#Min and Max number of fixations\n",
    "MIN_FIX = 10\n",
    "MAX_FIX = 1000\n",
    "\n",
    "# Min and Max fixations per second\n",
    "MIN_FPS = 0.5\n",
    "MAX_FPS = 8\n",
    "\n",
    "# Min Percentage HitsBlock\n",
    "MIN_HITS_BLOCK = 0.7\n",
    "\n",
    "\n",
    "# IQR bounds\n",
    "def iqr_bounds(series, k=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - k * IQR\n",
    "    upper = Q3 + k * IQR\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "# Compute IQR bounds\n",
    "fix_lo, fix_hi = iqr_bounds(df_results_overall[\"NumberOfFixations\"])\n",
    "fps_lo, fps_hi = iqr_bounds(df_results_overall[\"TrueFixationsPerSecond\"])\n",
    "\n",
    "print(\"Fix bounds:\", fix_lo, fix_hi)\n",
    "print(\"FPS bounds:\", fps_lo, fps_hi)\n",
    "\n",
    "mask_outliers = (\n",
    "    # Absolute checks\n",
    "        (df_results_overall[\"Time\"] > MAX_TIME) |\n",
    "\n",
    "        (df_results_overall[\"NumberOfFixations\"] < MIN_FIX) |\n",
    "        (df_results_overall[\"NumberOfFixations\"] > MAX_FIX) |\n",
    "\n",
    "        (df_results_overall[\"TrueFixationsPerSecond\"] < MIN_FPS) |\n",
    "        (df_results_overall[\"TrueFixationsPerSecond\"] > MAX_FPS) |\n",
    "\n",
    "        (df_results_overall[\"HitsBlock\"] < MIN_HITS_BLOCK) |\n",
    "\n",
    "        # IQR checks\n",
    "        (df_results_overall[\"NumberOfFixations\"] < fix_lo) |\n",
    "        (df_results_overall[\"NumberOfFixations\"] > fix_hi) |\n",
    "        (df_results_overall[\"TrueFixationsPerSecond\"] < fps_lo) |\n",
    "        (df_results_overall[\"TrueFixationsPerSecond\"] > fps_hi)\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Individual criterion masks\n",
    "masks = {\n",
    "    'Time > MAX_TIME': df_results_overall[\"Time\"] > MAX_TIME,\n",
    "    'NumFix < MIN_FIX': df_results_overall[\"NumberOfFixations\"] < MIN_FIX,\n",
    "    'NumFix > MAX_FIX': df_results_overall[\"NumberOfFixations\"] > MAX_FIX,\n",
    "    'FPS < MIN_FPS': df_results_overall[\"TrueFixationsPerSecond\"] < MIN_FPS,\n",
    "    'FPS > MAX_FPS': df_results_overall[\"TrueFixationsPerSecond\"] > MAX_FPS,\n",
    "    'HitsBlock < MIN': df_results_overall[\"HitsBlock\"] < MIN_HITS_BLOCK,\n",
    "    'NumFix < IQR_low': df_results_overall[\"NumberOfFixations\"] < fix_lo,\n",
    "    'NumFix > IQR_high': df_results_overall[\"NumberOfFixations\"] > fix_hi,\n",
    "    'FPS < IQR_low': df_results_overall[\"TrueFixationsPerSecond\"] < fps_lo,\n",
    "    'FPS > IQR_high': df_results_overall[\"TrueFixationsPerSecond\"] > fps_hi\n",
    "}\n",
    "\n",
    "# Print individual counts\n",
    "print(\"Individual criterion removal counts:\")\n",
    "for name, mask in masks.items():\n",
    "    count = mask.sum()\n",
    "    pct = (count / len(df_results_overall)) * 100\n",
    "    print(f\"{name:25s}: {count:4d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Total outliers\n",
    "total_outliers = mask_outliers.sum()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"{'Total outliers':25s}: {total_outliers:4d} ({(total_outliers / len(df_results_overall)) * 100:5.2f}%)\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Store in dataframe\n",
    "df_results_overall[\"Outlier\"] = mask_outliers\n",
    "\n",
    "# Check counts\n",
    "print(df_results_overall[\"Outlier\"].value_counts())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall[\n",
    "    ['ID', 'Task', 'CorrectAnswer', 'Time', 'NumberOfFixations', 'TrueFixationsPerSecond', 'HitsBlock', 'Outlier',\n",
    "     'HitsType', 'HitsReturnType', 'Meaningful', 'TypeAnnotation']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# drop outliers\n",
    "df_results_overall = df_results_overall[~df_results_overall[\"Outlier\"]].copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# df with only id, task, correctanswer, time, number of fixations, true fixations per second\n",
    "df_analysis = df_results_overall[\n",
    "    ['ID', 'Task', 'CorrectAnswer', 'Time', 'NumberOfFixations', 'TrueFixationsPerSecond', 'HitsBlock', 'Difficulty',\n",
    "     'HitsType', 'HitsReturnType', 'Meaningful', 'TypeAnnotation']]\n",
    "df_analysis"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(len(df_analysis['ID'].value_counts()))\n",
    "# print number of particpants in meaningful and obfuscated group\n",
    "print(\"Number of participants in meaningful group:\", df_analysis.loc[df_analysis[\"Meaningful\"] == True, \"ID\"].nunique())\n",
    "print(\"Unique participants in obfuscated group:\", df_analysis.loc[df_analysis[\"Meaningful\"] == False, \"ID\"].nunique())\n",
    "\n",
    "# print number of tasks for meaningful and obfuscated group\n",
    "print(f'Number of tasks in meaningful group: {len(df_analysis[df_analysis[\"Meaningful\"] == True])}')\n",
    "print(f'Number of tasks in obfuscated group: {len(df_analysis[df_analysis[\"Meaningful\"] == False])}')\n",
    "\n",
    "# print number of tasks in TA and NoTA group\n",
    "print(f'Number of tasks in TA group: {len(df_analysis[df_analysis[\"TypeAnnotation\"] == True])}')\n",
    "print(f'Number of tasks in NoTA group: {len(df_analysis[df_analysis[\"TypeAnnotation\"] == False])}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# mean time and sd\n",
    "mean_time = df_analysis['Time'].mean()\n",
    "std_time = df_analysis['Time'].std()\n",
    "print(f'Mean Time: {mean_time:.1f} seconds, SD: {std_time:.1f} seconds')\n",
    "\n",
    "# mean time meaningful\n",
    "mean_time_meaningful = df_analysis[df_analysis['Meaningful'] == True]['Time'].mean()\n",
    "std_time_meaningful = df_analysis[df_analysis['Meaningful'] == True]['Time'].std()\n",
    "print(f'Mean Time Meaningful: {mean_time_meaningful:.1f} seconds, SD: {std_time_meaningful:.1f} seconds')\n",
    "\n",
    "# mean time obfuscated\n",
    "mean_time_obfuscated = df_analysis[df_analysis['Meaningful'] == False]['Time'].mean()\n",
    "std_time_obfuscated = df_analysis[df_analysis['Meaningful'] == False]['Time'].std()\n",
    "print(f'Mean Time Obfuscated: {mean_time_obfuscated:.1f} seconds, SD: {std_time_obfuscated:.1f} seconds')\n",
    "\n",
    "# number of meaningful tasks correct\n",
    "num_meaningful_tasks = len(df_analysis[df_analysis['Meaningful'] == True])\n",
    "num_correct_meaningful = len(df_analysis[(df_analysis['Meaningful'] == True) & (df_analysis['CorrectAnswer'] == True)])\n",
    "print(\n",
    "    f'Number of Correct Meaningful Tasks: {num_correct_meaningful} out of {num_meaningful_tasks} ({(num_correct_meaningful / num_meaningful_tasks) * 100:.1f}%)')\n",
    "\n",
    "# number of obfuscated tasks correct\n",
    "num_obfuscated_tasks = len(df_analysis[df_analysis['Meaningful'] == False])\n",
    "num_correct_obfuscated = len(df_analysis[(df_analysis['Meaningful'] == False) & (df_analysis['CorrectAnswer'] == True)])\n",
    "print(\n",
    "    f'Number of Correct Obfuscated Tasks: {num_correct_obfuscated} out of {num_obfuscated_tasks} ({(num_correct_obfuscated / num_obfuscated_tasks) * 100:.1f}%)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AOI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall['DifficultyNumeric'] = df_results_overall['Difficulty'].dropna().astype(float)\n",
    "df_results_overall['DifficultyNumeric'] = df_results_overall['DifficultyNumeric'].apply(lambda x: x / 5)\n",
    "df_results_overall['OverallExperienceNumeric'] = df_results_overall['OverallExperience'].dropna().astype(float)\n",
    "df_results_overall['OverallExperienceNumeric'] = df_results_overall['OverallExperienceNumeric'].apply(lambda x: x / 5)\n",
    "df_results_overall['HitsTypePct'] = df_results_overall['HitsType'] * 100\n",
    "df_results_overall['HitsReturnTypePct'] = df_results_overall['HitsReturnType'] * 100\n",
    "df_results_overall['HitsTypeCombinedPct'] = df_results_overall['HitsTypePct'] + df_results_overall['HitsReturnTypePct']\n",
    "\n",
    "# Aggregate only snippets with TA\n",
    "df_type = df_results_overall[df_results_overall['TypeAnnotation'] == True]\n",
    "\n",
    "# Sort tasks by HitsTypeCombinedPct\n",
    "df_type = df_type.sort_values('HitsTypeCombinedPct', ascending=False)\n",
    "\n",
    "# Compute mean per participant\n",
    "mean_order = (\n",
    "    df_type.groupby('ID')['HitsTypeCombinedPct']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# map each participant in meaningful_participants to another id for the plot starting with M like M01 M02 etc. and for obfuscated use L03 L04 etc.\n",
    "# create a plot-only ID mapping (Mxx if any Meaningful==True for that ID, otherwise Lxx)\n",
    "df_plot = df_type.copy()\n",
    "unique_ids = mean_order.tolist()\n",
    "\n",
    "label_map = {}\n",
    "m_labels = []\n",
    "l_labels = []\n",
    "count = 1\n",
    "for uid in unique_ids:\n",
    "    is_meaningful = df_plot.loc[df_plot['ID'] == uid, 'Meaningful'].fillna(False).any()\n",
    "    if is_meaningful:\n",
    "        label_map[uid] = f\"P{count:02d}\"\n",
    "        m_labels.append(f\"P{count:02d}\")\n",
    "        count += 1\n",
    "    else:\n",
    "        label_map[uid] = f\"P{count:02d}\"\n",
    "        l_labels.append(f\"P{count:02d}\")\n",
    "        count += 1\n",
    "\n",
    "# add PlotID column and build order for plotting\n",
    "df_plot['ID'] = df_plot['ID'].map(label_map)\n",
    "\n",
    "# prepare order with a separator between M and L\n",
    "sep_label = ''  # empty string used as separator (will produce a tick but no data)\n",
    "plot_order = m_labels + [sep_label] + l_labels\n",
    "\n",
    "# add one dummy row per hue with the separator label and NaN y so a gap appears\n",
    "dummy_rows = []\n",
    "for hue_val in df_plot['TAComprehension'].dropna().unique():\n",
    "    dummy_rows.append({\n",
    "        'PlotID': sep_label,\n",
    "        'HitsTypeCombinedPct': np.nan,\n",
    "        'TAComprehension': hue_val\n",
    "    })\n",
    "df_plot_with_sep = pd.concat([df_plot, pd.DataFrame(dummy_rows)], ignore_index=True)\n",
    "\n",
    "# --- Describe data before filtering participants ---\n",
    "\n",
    "# Count number of datapoints per participant\n",
    "df_counts = df_plot_with_sep.copy()\n",
    "\n",
    "# Plot using the sorted order\n",
    "# Publication styling\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper', font_scale=1.05)\n",
    "\n",
    "#palette = {1.0: '#96a3a6', 0.0: '#aaa97a'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "\n",
    "# point estimates use color c9d3d6 for all points\n",
    "sns.pointplot(\n",
    "    data=df_plot,\n",
    "    x='ID',\n",
    "    y='HitsTypeCombinedPct',\n",
    "    color='#8CA8AC',\n",
    "    #hue='TAComprehension',\n",
    "    order=plot_order,\n",
    "    dodge=0,\n",
    "    linestyle='none',\n",
    "    errorbar=None,\n",
    "    #palette=palette,\n",
    "    markers='_',\n",
    "    markersize=15,\n",
    "    markeredgewidth=2,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# overlay raw observations (small, semi-transparent)\n",
    "sns.stripplot(\n",
    "    data=df_plot,\n",
    "    x='ID',\n",
    "    y='HitsTypeCombinedPct',\n",
    "    color='#8CA8AC',\n",
    "    #hue='TAComprehension',\n",
    "    order=plot_order,\n",
    "    dodge=False,\n",
    "    jitter=0.15,\n",
    "    size=4,\n",
    "    #palette=palette,\n",
    "    linewidth=0,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# remove duplicate legend entries\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# keep only the first pair of legend entries (one per hue)\n",
    "unique = {}\n",
    "for h, l in zip(handles, labels):\n",
    "    if l not in unique:\n",
    "        unique[l] = h\n",
    "ax.legend(unique.values(), unique.keys(), title='Are Type Annotations Helpful?', frameon=False, loc='upper right',\n",
    "          fontsize=9, bbox_to_anchor=(1, 0.85))\n",
    "\n",
    "# no legend\n",
    "ax.get_legend().remove()\n",
    "\n",
    "# Add group labels above each participant group (aligned)\n",
    "# Current y-axis limits\n",
    "ymin, ymax = ax.get_ylim()\n",
    "\n",
    "# Vertical placement for labels and lines\n",
    "label_y = ymax * 1.1\n",
    "line_y = ymax * 1.08\n",
    "\n",
    "# Compute exact start/end indices for groups\n",
    "m_start = 0\n",
    "m_end = len(m_labels) - 1\n",
    "l_start = len(m_labels) + 1  # skip separator tick\n",
    "l_end = l_start + len(l_labels) - 1\n",
    "\n",
    "# Compute group centers for label placement\n",
    "m_center = (m_start + m_end) / 2\n",
    "l_center = (l_start + l_end) / 2\n",
    "\n",
    "# Add text labels\n",
    "ax.text(m_center, label_y, 'Meaningful',\n",
    "        ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "ax.text(l_center, label_y, 'Obfuscated',\n",
    "        ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Add horizontal lines\n",
    "ax.hlines(line_y, m_start, m_end, color='gray', lw=0.8)\n",
    "ax.hlines(line_y, l_start, l_end, color='gray', lw=0.8)\n",
    "\n",
    "ax.set_ylim(ymin, ymax * 1.15)\n",
    "\n",
    "# aesthetics for publication\n",
    "ax.set_xlabel('Participant')\n",
    "ax.set_ylabel('Fixation % on Type Annotations')\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine(trim=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/FixationsOnTypePart.pdf', dpi=300, bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "task_order = (\n",
    "    df_type.groupby('Task')['HitsTypeCombinedPct']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "# point estimates\n",
    "sns.pointplot(\n",
    "    data=df_type,\n",
    "    x='Task',\n",
    "    y='HitsTypeCombinedPct',\n",
    "    color='#8CA8AC',\n",
    "    dodge=False,\n",
    "    order=task_order,\n",
    "    linestyle='none',\n",
    "    errorbar=None,\n",
    "    markers='_',\n",
    "    markersize=15,\n",
    "    markeredgewidth=4,\n",
    ")\n",
    "\n",
    "sns.stripplot(data=df_type,\n",
    "              x='Task',\n",
    "              y='HitsTypeCombinedPct',\n",
    "              color='#8CA8AC',\n",
    "              order=task_order,\n",
    "              jitter=0.15,\n",
    "              size=6,\n",
    "              dodge=False,\n",
    "              alpha=0.8)\n",
    "plt.ylabel('Fixation % on Type Annotations', fontsize=20)\n",
    "plt.xlabel('')\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xticks(rotation=80,\n",
    "           fontsize=20,\n",
    "           ha='right',\n",
    "           rotation_mode='anchor',  # prevents rotation from shifting tick positions\n",
    "           )\n",
    "plt.savefig('output/FixationsOnTypeSnippet.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Aggregate per task\n",
    "df_task = df_type.groupby('Task')[['HitsTypeCombinedPct', 'DifficultyNumeric', 'TAComprehension', 'YearsProgramming',\n",
    "                                   'OverallExperienceNumeric']].mean().reset_index()\n",
    "\n",
    "df_task_sorted = df_task.sort_values('HitsTypeCombinedPct', ascending=False)\n",
    "\n",
    "# HitsType + Difficulty side by side\n",
    "df_task_heat = df_task_sorted.set_index('Task')[['HitsTypeCombinedPct', 'DifficultyNumeric']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6), gridspec_kw={'width_ratios': [1, 1]})\n",
    "\n",
    "# HitsType heatmap (Blue)\n",
    "sns.heatmap(df_task_heat[['HitsTypeCombinedPct']], annot=True, fmt=\".1f\", cmap='Blues', cbar=True, ax=axes[0])\n",
    "axes[0].set_title('HitsTypeCombinedPct Fixations (%)')\n",
    "axes[0].set_ylabel('Task')\n",
    "axes[0].set_xlabel('')\n",
    "\n",
    "# Difficulty heatmap (Red)\n",
    "sns.heatmap(df_task_heat[['DifficultyNumeric']], annot=True, fmt=\".2f\", cmap='Reds', cbar=True, ax=axes[1])\n",
    "axes[1].set_title('Difficulty (Normalized)')\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_xlabel('')\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Aggregate per participant\n",
    "df_participant = df_type.groupby('ID')[\n",
    "    ['HitsTypeCombinedPct', 'DifficultyNumeric', 'TAComprehension', 'YearsProgramming',\n",
    "     'OverallExperienceNumeric']].mean().reset_index()\n",
    "\n",
    "# HitsType vs Difficulty\n",
    "df_corr = df_participant[['HitsTypeCombinedPct', 'DifficultyNumeric']].dropna()\n",
    "\n",
    "# Spearman correlation\n",
    "spearman_r, spearman_p = spearmanr(df_corr['HitsTypeCombinedPct'], df_corr['DifficultyNumeric'])\n",
    "print(f\"Spearman correlation: ρ = {spearman_r:.3f}, p = {spearman_p:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.regplot(\n",
    "    data=df_corr,\n",
    "    x='DifficultyNumeric',\n",
    "    y='HitsTypeCombinedPct',\n",
    "    ci=None,\n",
    ")\n",
    "# Force axes to start from 0\n",
    "plt.xlim(left=0, right=1.0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Correlation between Difficulty and Type-related Fixations')\n",
    "plt.xlabel('Difficulty')\n",
    "plt.ylabel('Fixation % (Type + Return Type)')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "df_corr = df_participant[['HitsTypeCombinedPct', 'YearsProgramming']].dropna()\n",
    "\n",
    "# Spearman correlation\n",
    "spearman_r, spearman_p = kendalltau(df_corr['HitsTypeCombinedPct'], df_corr['YearsProgramming'])\n",
    "print(f\"Spearman correlation: ρ = {spearman_r:.3f}, p = {spearman_p:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.regplot(\n",
    "    data=df_corr,\n",
    "    x='YearsProgramming',\n",
    "    y='HitsTypeCombinedPct',\n",
    "    ci=None,\n",
    ")\n",
    "# Force axes to start from 0\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Correlation between Programming Experience and Type-related Fixations')\n",
    "plt.xlabel('Years of Programming Experience')\n",
    "plt.ylabel('Fixation % (Type + Return Type)')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_personal_information.CourseOfStudy.value_counts(normalize=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.countplot(df_personal_information, x='CourseOfStudy')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People Distributions and Data\n",
    "\n",
    "1. The age\n",
    "2. Their experience\n",
    "3. Their Course of Study"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# average age and standard deviation\n",
    "mean_age = df_personal_information['Age'].mean()\n",
    "std_age = df_personal_information['Age'].std()\n",
    "print(f'Mean Age: {mean_age:.1f}, SD: {std_age:.1f}')\n",
    "sns.boxplot(data=df_personal_information, y='Age', )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(data=df_personal_information, y='Age', hue='Meaningful', gap=.1)\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "group_stats_age = df_personal_information.groupby('Meaningful')['Age'].agg(['mean', 'std'])\n",
    "\n",
    "# Print values\n",
    "for group, group_data in group_stats_age.iterrows():\n",
    "    print(f\"{group} -> Mean: {group_data['mean']:.2f}, SD: {group_data['std']:.2f}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The courses of study"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.countplot(data=df_personal_information, x='CourseOfStudy', hue='Gender')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of semesters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.barplot(data=df_personal_information, x='CourseOfStudy', hue='Gender', y='Semester')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of semesters in general"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# mean semester and standard deviation\n",
    "mean_semester = df_personal_information['Semester'].mean()\n",
    "std_semester = df_personal_information['Semester'].std()\n",
    "print(f'Mean Semester: {mean_semester:.2f}, SD: {std_semester:.2f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(data=df_personal_information, y='Semester', hue='Meaningful', gap=.1, )\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "group_stats = df_personal_information.groupby('Meaningful')['Semester'].agg(['mean', 'std'])\n",
    "\n",
    "# Print values\n",
    "for group, group_data in group_stats.iterrows():\n",
    "    print(f\"{group} -> Mean: {group_data['mean']:.2f}, SD: {group_data['std']:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classmates and overall experience"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(data=df_personal_information, y='OverallExperience', x='Semester', hue='Meaningful', gap=.1,\n",
    "            # order=['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced']\n",
    "            )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classmates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(data=df_personal_information,\n",
    "            y='Classmates',\n",
    "            x='Semester',\n",
    "            hue='Meaningful',\n",
    "            gap=.1,\n",
    "            # order=['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'],\n",
    "            )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverallExperience by ProgrammingLately"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.catplot(data=df_personal_information,\n",
    "            x='ProgrammingLately',\n",
    "            y='Classmates',\n",
    "            hue='Meaningful',\n",
    "            # gap=.1,\n",
    "            order=['Daily', 'Weekly', 'Monthly', 'Yearly'],\n",
    "            kind='swarm'\n",
    "            )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Data for the personal information"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'Age:\\n{df_personal_information[\"Age\"].describe()}\\n')\n",
    "print(f'Semester:\\n{df_personal_information[\"Semester\"].describe()}\\n')\n",
    "print(f'YearsProgramming:\\n{df_personal_information[\"YearsProgramming\"].describe()}\\n')\n",
    "print(f'Gender:\\n{df_personal_information.groupby(\"Gender\").agg({\"Gender\": \"count\"})}\\n')\n",
    "print(f'Classmates:\\n{df_personal_information.groupby(\"Classmates\").agg({\"Classmates\": \"count\"})}\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Data for personal information divided by Meaningful Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'Age:\\n{df_personal_information.groupby(\"Meaningful\").agg({\"Age\": [\"mean\", \"std\"]})}')\n",
    "print(f'Semester:\\n{df_personal_information.groupby(\"Meaningful\").agg({\"Semester\": [\"mean\", \"std\"]})}\\n')\n",
    "print(\n",
    "    f'YearsProgramming:\\n{df_personal_information.groupby(\"Meaningful\").agg({\"YearsProgramming\": [\"mean\", \"std\"]})}\\n')\n",
    "print(f'Gender:\\n{df_personal_information.groupby([\"Meaningful\", \"Gender\"]).agg({\"Gender\": \"count\"})}\\n')\n",
    "print(f'EyeSight:\\n{df_personal_information.groupby([\"Meaningful\", \"Eyesight\"]).agg({\"Eyesight\": \"count\"})}\\n')\n",
    "print(f'StudyBefore:\\n{df_personal_information.groupby([\"Meaningful\", \"StudyBefore\"]).agg({\"StudyBefore\": \"count\"})}\\n')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficacy for every participant:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = df_results_overall.groupby(['ID', 'TypeAnnotation']).agg({'Time': 'sum', 'CorrectAnswer': 'count'})\n",
    "df['efficacy'] = df.CorrectAnswer / (df.Time / 60)\n",
    "# df.columns.droplevel(0)\n",
    "# print(df)\n",
    "for index, row in df.iterrows():\n",
    "    print(f'Efficacy of Participant {index}: {row.CorrectAnswer / (row.Time / 60)}')\n",
    "\n",
    "sns.violinplot(df, y='efficacy', hue='TypeAnnotation', cut=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Methods one after the other"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "def compute_statistics(data: DataFrame, with_TA, without_TA, input_type: str = 'continuous',\n",
    "                       output_type: str = 'contiuous', population: str = 'within', p_value: float = 0.05):\n",
    "    # input_dependence: str = 'independent',\n",
    "\n",
    "    # check for normality\n",
    "    with_TA_normality = stats.shapiro(with_TA)\n",
    "    without_TA_normality = stats.shapiro(without_TA)\n",
    "\n",
    "    # print(with_TA)\n",
    "    # print(without_TA)\n",
    "\n",
    "    # check for homogeneity of variance\n",
    "    homogeneity_of_variance = stats.levene(with_TA, without_TA)\n",
    "    # print(homogeneity_of_variance)\n",
    "\n",
    "    if with_TA_normality.pvalue > p_value and without_TA_normality.pvalue > p_value:\n",
    "        print(\n",
    "            f'Both groups are normally distributed with pvalues {with_TA_normality.pvalue} and {without_TA_normality.pvalue}')\n",
    "    else:\n",
    "        print(\n",
    "            f'At least one group is not normally distributed with pvalues {with_TA_normality.pvalue} and {without_TA_normality.pvalue}')\n",
    "\n",
    "    if homogeneity_of_variance.pvalue > p_value:\n",
    "        print(f'Both groups have homogeneity of variance with pvalue {homogeneity_of_variance.pvalue}')\n",
    "    else:\n",
    "        print(f'Both groups do not have homogeneity of variance with pvalue {homogeneity_of_variance.pvalue}')\n",
    "\n",
    "    if with_TA_normality.pvalue > p_value and without_TA_normality.pvalue > p_value and homogeneity_of_variance.pvalue > p_value:\n",
    "        if population == 'within':\n",
    "            if input_type == 'categorical' and output_type == 'continuous':\n",
    "                print(f'TTEST_REL: {stats.ttest_rel(without_TA, with_TA)}')\n",
    "            elif input_type == 'categorical' and output_type == 'categorical':\n",
    "                print(f'CHISQUARE: {stats.chisquare(without_TA, with_TA)}')\n",
    "            elif len(with_TA) == len(without_TA):\n",
    "                print(f'WILCOXON TWO-SIDED: {stats.wilcoxon(without_TA, with_TA)}')\n",
    "                print(f'WILCOXON ONE-SIDED Greater: {stats.wilcoxon(without_TA, with_TA, alternative=\"greater\")}')\n",
    "                print(f'WILCOXON ONE-SIDED Less: {stats.wilcoxon(without_TA, with_TA, alternative=\"less\")}')\n",
    "            else:\n",
    "                print(f'MANNWHITNEYU: {stats.mannwhitneyu(without_TA, with_TA)}')\n",
    "\n",
    "        else:\n",
    "            if input_type == 'categorical' and output_type == 'continuous':\n",
    "                print(f'TTEST_IND: {stats.ttest_ind(without_TA, with_TA)}')\n",
    "            else:\n",
    "                print(f'MANNWHITNEYU: {stats.mannwhitneyu(without_TA, with_TA)}')\n",
    "\n",
    "    else:\n",
    "        if population == 'within' and len(with_TA) == len(without_TA):\n",
    "            print(f'WILCOXON TWO-SIDED: {stats.wilcoxon(without_TA, with_TA)}')\n",
    "        elif input_type == 'categorical' and output_type == 'categorical':\n",
    "            print(f'CHISQUARE: {stats.chi2_contingency(without_TA, with_TA)}')\n",
    "        else:\n",
    "            u_statistic, p_value = stats.mannwhitneyu(without_TA, with_TA)\n",
    "            print(f'The U-statistic is {u_statistic} and the p-value is {p_value}')\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table for Type Annotations and Correctness RQ 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do everything correctly for time\n",
    "\n",
    "1. Check for normality\n",
    "2. Check for the variances\n",
    "3. then check for wilcoxon two-sided\n",
    "4. then check for wilcoxon one-sided\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "table_data: dict[str, list[str] | str] = {\n",
    "    'Time': ['mean', 'std'],\n",
    "    'CorrectAnswer': ['sum', 'count'],\n",
    "}\n",
    "\n",
    "table: DataFrame = df_results_overall.groupby(['Task']).agg(table_data)\n",
    "\n",
    "TA_mean = df_results_overall.query('TypeAnnotation == True')['Time']\n",
    "NoTA_mean = df_results_overall.query('TypeAnnotation == False')['Time']\n",
    "\n",
    "# qq = stats.probplot(TA_mean, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Normal Q-Q plot\")\n",
    "# plt.show()\n",
    "\n",
    "data = compute_statistics(table, TA_mean, NoTA_mean, input_type='categorical', output_type='continuous',\n",
    "                          population='within')\n",
    "\n",
    "# print(f'{TA_mean.describe()} and {NoTA_mean.describe()}')\n",
    "\n",
    "# qq = stats.probplot(NoTA_mean, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Normal Q-Q plot\")\n",
    "# plt.show()\n",
    "\n",
    "print(f'Normality of TA:\\n{stats.shapiro(TA_mean)}')\n",
    "print(f'Normality of NoTA:\\n{stats.shapiro(NoTA_mean)}')\n",
    "\n",
    "print(f'Levene Test:\\n{stats.levene(TA_mean, NoTA_mean)}')\n",
    "\n",
    "# print(f'WILCOXON:\\n{stats.wilcoxon(NoTA_mean, TA_mean)}')\n",
    "# print(f'WILCOXON Longer with TA:\\n{stats.wilcoxon(NoTA_mean, TA_mean, alternative=\"greater\")}')\n",
    "# print(f'WILCOXON Shorter with TA:\\n{stats.wilcoxon(NoTA_mean, TA_mean, alternative=\"less\")}')\n",
    "for task_tuple, x in table.iterrows():\n",
    "    print(\n",
    "        f'{task_tuple} & {x.CorrectAnswer[\"sum\"]}/{x.CorrectAnswer[\"count\"]} ({round((x.CorrectAnswer[\"sum\"] / x.CorrectAnswer[\"count\"]) * 100)}\\%) & {round(x.Time[\"mean\"], 2)} $\\pm$ {round(x.Time[\"std\"], 2)}\\\\\\\\')\n",
    "\n",
    "print(\n",
    "    f'{df_results_overall.Time.mean()} + {df_results_overall.Time.std()} | {df_results_overall.CorrectAnswer.sum()}/{df_results_overall.CorrectAnswer.count()} ({df_results_overall.CorrectAnswer.sum() / df_results_overall.CorrectAnswer.count()})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(data=df_results_overall, y='Time', x='TypeAnnotation')\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "group_stats = df_results_overall.groupby('TypeAnnotation')['Time'].agg(['mean', 'std'])\n",
    "\n",
    "# Print values\n",
    "for group, group_data in group_stats.iterrows():\n",
    "    print(f\"{group} -> Mean: {group_data['mean']:.2f}, SD: {group_data['std']:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above for the correctness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# percentage_TA = table.query(\"TypeAnnotation == True\")[\"Correctness\"]\n",
    "# percentage_NoTA = table.query(\"TypeAnnotation == False\")[\"Correctness\"]\n",
    "\n",
    "# please create a contingency table for CorrectAnswer on TypeAnnotation\n",
    "\n",
    "contingency_table = pd.crosstab(df_results_overall['TypeAnnotation'], df_results_overall['CorrectAnswer'])\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)\n",
    "\n",
    "try:\n",
    "    data = compute_statistics(DataFrame(), df_results_overall.query(\"TypeAnnotation == True\")[\"CorrectAnswer\"],\n",
    "                              df_results_overall.query(\"TypeAnnotation == False\")[\"CorrectAnswer\"],\n",
    "                              input_type='categorical', output_type='categorical', population='within')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(df_results_overall.groupby('TypeAnnotation')['CorrectAnswer'].agg(['sum', 'count']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculate correctness values for TA and NoTA\n",
    "ta_correct = df_results_overall[df_results_overall['TypeAnnotation'] == True]['CorrectAnswer'].sum()\n",
    "ta_total = len(df_results_overall[df_results_overall['TypeAnnotation'] == True])\n",
    "ta_percent = round((ta_correct / ta_total) * 100)\n",
    "\n",
    "nota_correct = df_results_overall[df_results_overall['TypeAnnotation'] == False]['CorrectAnswer'].sum()\n",
    "nota_total = len(df_results_overall[df_results_overall['TypeAnnotation'] == False])\n",
    "nota_percent = round((nota_correct / nota_total) * 100)\n",
    "\n",
    "all_correct = df_results_overall['CorrectAnswer'].sum()\n",
    "all_total = len(df_results_overall)\n",
    "all_percent = round((all_correct / all_total) * 100)\n",
    "\n",
    "# Print LaTeX commands\n",
    "print(\"% The general correctness values for TA and without\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessAllTATotal}}{{{ta_total}}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessAllTACorrect}}{{{ta_correct}}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessTAPercent}}{{{ta_percent}\\\\%}}\")\n",
    "print()\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessAllNoTATotal}}{{{nota_total}}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessAllNoTACorrect}}{{{nota_correct}}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessNoTAPercent}}{{{nota_percent}\\\\%}}\")\n",
    "print()\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessAllTotal}}{{{all_total}}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessAllCorrect}}{{{all_correct}}}\")\n",
    "print(f\"\\\\newcommand{{\\\\rqOneOneGeneralCorrectnessPercent}}{{{all_percent}\\\\%}}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures for RQ 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we should check for VerticalNext, VerticalLater, Regression, HorizontalLater, and LineRegression\n",
    "\n",
    "But first let's create the table to analyze"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "list_measure: list[str] = ['VerticalNext', 'VerticalLater', 'Regression', 'HorizontalLater', 'LineRegression',\n",
    "                           'StoryOrder_Naive_Score', 'StoryOrder_Dynamic_Score', 'StoryOrder_Dynamic_Repetitions',\n",
    "                           'SaccadeLength']\n",
    "\n",
    "table: DataFrame = df_results_overall.groupby(['Task', 'TypeAnnotation']).agg(table_data)\n",
    "table.dropna(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for measure in list_measure:\n",
    "    print(f'\\nMeasure: {measure}')\n",
    "    data = compute_statistics(table, df_results_overall.query(\"TypeAnnotation == True\")[measure].dropna(),\n",
    "                              df_results_overall.query(\"TypeAnnotation == False\")[measure].dropna(),\n",
    "                              input_type='categorical', output_type='continuous', population='within')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures for RQ 1.3\n",
    "\n",
    "This should include:\n",
    "\n",
    "1. The difficulty for each of the snippets as giving by the people.\n",
    "2. Did the Type Annotations help with the snippets?\n",
    "3. Do Type Annotations help in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a likert scale analysis of the difficulty "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df1 = df_results_overall.groupby('TypeAnnotation')['Difficulty'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Difficulty',\n",
    "            hue='TypeAnnotation',\n",
    "            data=df1,\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            #legend=False,\n",
    "            )\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "# plt.title('Difficulty grouped by Type Annotation')\n",
    "# plt.legend(title='Type Annotation', loc='upper right', labels=['Non-Annotated', 'Annotated'])\n",
    "# TODO: uncomment the following line to save the figure\n",
    "# plt.savefig(f'{figure_path}/rq13difficultyLikertCatPlot.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with meaningful"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df1 = df_results_overall.groupby(['Meaningful', 'TypeAnnotation'])['Difficulty'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "print(df1[df1['Meaningful'] == True])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Difficulty',\n",
    "            hue='TypeAnnotation',\n",
    "            data=df1[df1['Meaningful'] == True],\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            )\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# TODO: uncomment the following line to save the figure\n",
    "#plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df1 = df_results_overall.groupby(['Meaningful', 'TypeAnnotation'])['Difficulty'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# g = sns.FacetGrid(df1, col='Meaningful', hue='TypeAnnotation')\n",
    "# g.map(sns.stripplot, 'Difficulty', 'percent', kind='bar')\n",
    "sns.catplot(x='Difficulty',\n",
    "            hue='TypeAnnotation',\n",
    "            data=df1,\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            col='Meaningful',\n",
    "            )\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# TODO: uncomment the following line to save the figure\n",
    "#plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotCombined.pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now both?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Difficulty',\n",
    "            hue='TypeAnnotation',\n",
    "            data=df1[df1['Meaningful'] == False],\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            )\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Easy', 'Easy', 'Neutral', 'Difficult', 'Very Difficult'])\n",
    "plt.title('Difficulty grouped by Type Annotation')\n",
    "# TODO: uncomment the following line to save the figure\n",
    "#plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotObfuscated.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we also find a statistical significance for this? using the chisquare test?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "contingency_table = pd.crosstab(df_results_overall['TypeAnnotation'], df_results_overall['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cont_df = df_results_overall[df_results_overall['Meaningful'] == True]\n",
    "contingency_table = pd.crosstab(cont_df['TypeAnnotation'], cont_df['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cont_df = df_results_overall[df_results_overall['Meaningful'] == False]\n",
    "contingency_table = pd.crosstab(cont_df['TypeAnnotation'], cont_df['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "contingency_table = pd.crosstab(df_results_overall['Meaningful'], df_results_overall['Difficulty'])\n",
    "\n",
    "print(stats.chi2_contingency(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about some significance for the Comprehension within the study?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "contingency_table = pd.crosstab(df_personal_information['TAComprehension'],\n",
    "                                df_personal_information['TAComprehensionGeneral'])\n",
    "\n",
    "print(stats.fisher_exact(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "contingency_table = pd.crosstab(df_personal_information['TAComprehension'], df_personal_information['Meaningful'])\n",
    "\n",
    "print(stats.fisher_exact(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "contingency_table = pd.crosstab(df_personal_information['TAComprehensionGeneral'],\n",
    "                                df_personal_information['Meaningful'])\n",
    "\n",
    "print(stats.fisher_exact(contingency_table))\n",
    "print(contingency_table)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the TA help with comprehension?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "table_data: dict[str | list[str]] = {\n",
    "    # 'Difficulty': ['mean'],\n",
    "    'TAComprehension': [],\n",
    "    # 'TAComprehensionGeneral': [],\n",
    "}\n",
    "print(df_personal_information.query('TAComprehension == False')['ID'])\n",
    "df_personal_information['TAComprehension'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TA help with comprehension for Meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_personal_information.groupby('Meaningful')['TAComprehension'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "stats.chisquare(df_personal_information.groupby('Meaningful')['TAComprehension'].value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do TA help in general?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_personal_information.query('TAComprehensionGeneral == False')['ID'])\n",
    "\n",
    "df_personal_information['TAComprehensionGeneral'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_personal_information.groupby('Meaningful')['TAComprehensionGeneral'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the plot for the difficulty of the snippets grouped by meaningful"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(df_results_overall.query('Meaningful == True'), x='Task', y=\"Difficulty\", hue=\"TypeAnnotation\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "# TODO: uncomment the following line to save the figure\n",
    "#plt.savefig(f'{figure_path}/difficultyrq23_Meaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.boxplot(df_results_overall.query('Meaningful == False'), x='Task', y=\"Difficulty\", hue=\"TypeAnnotation\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "# TODO: uncomment the following line to save the figure\n",
    "# plt.savefig(f'{figure_path}/difficultyrq23_Obfuscated.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures for RQ 2.1\n",
    "\n",
    "This shall include all information for the bahavioral measures as seen above"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "table_data: dict[str, list[str] | str] = {\n",
    "    'Time': ['mean', 'std'],\n",
    "    # 'CorrectAnswer': ['sum', 'count'],\n",
    "}\n",
    "\n",
    "table: DataFrame = df_results_overall.groupby(['Task', 'TypeAnnotation', 'Meaningful']).agg(table_data)\n",
    "\n",
    "# TA_L_mean = table.query(\"TypeAnnotation == True and Meaningful == False\")[\"Time\"][\"mean\"]\n",
    "# NoTA_L_mean = table.query(\"TypeAnnotation == False and Meaningful == False\")[\"Time\"][\"mean\"]\n",
    "# TA_M_mean = table.query('TypeAnnotation == True and Meaningful == True')['Time']['mean']\n",
    "# NoTA_M_mean = table.query('TypeAnnotation == False and Meaningful == True')['Time']['mean']\n",
    "\n",
    "TA_L = df_results_overall.query(\"TypeAnnotation == True and Meaningful == False\")[\"Time\"]\n",
    "NoTA_L = df_results_overall.query(\"TypeAnnotation == False and Meaningful == False\")[\"Time\"]\n",
    "TA_M = df_results_overall.query(\"TypeAnnotation == True and Meaningful == True\")[\"Time\"]\n",
    "NoTA_M = df_results_overall.query(\"TypeAnnotation == False and Meaningful == True\")[\"Time\"]\n",
    "\n",
    "print(f'NORMALITY:\\n{pg.normality(df_results_overall, dv=\"Time\", group=\"TypeAnnotation\")}')\n",
    "print(f'NORMALITY:\\n{pg.normality(df_results_overall, dv=\"Time\", group=\"Meaningful\")}')\n",
    "\n",
    "print(f'Levene:\\n{stats.levene(TA_L, NoTA_M, TA_M, NoTA_L)}')\n",
    "\n",
    "print(f'{df_results_overall.query(\"Time == 0\")}')\n",
    "\n",
    "# print(f'Obfuscated: Wilcoxon NoTA -> TA:\\n{stats.wilcoxon(NoTA_L_mean, TA_L_mean)}')\n",
    "# print(f'Obfuscated: MannWhitneyU NoTA -> Meaningful NoTA:\\n{stats.mannwhitneyu(NoTA_L_mean, NoTA_M_mean)}')\n",
    "# print(f'Obfuscated: MannWhitneyU NoTA -> Meaningful TA:\\n{stats.mannwhitneyu(NoTA_L_mean, TA_M_mean)}')\n",
    "# print(f'Obfuscated: MannWhitneyU TA -> Meaningful TA:\\n{stats.mannwhitneyu(TA_L_mean, TA_M_mean)}')\n",
    "# print(f'Meaningful: MannWhitneyU NoTA -> Obfuscated TA:\\n{stats.mannwhitneyu(NoTA_M_mean, TA_L_mean)}')\n",
    "# print(f'Meaningful: Wilcoxon NoTA -> TA:\\n{stats.wilcoxon(NoTA_M_mean, TA_M_mean)}')\n",
    "print(pg.sphericity(df_results_overall, dv='Time', within=['Meaningful'], subject='ID'))\n",
    "print(pg.sphericity(df_results_overall, dv='Time', within=['TypeAnnotation'], subject='ID'))\n",
    "\n",
    "print(pg.sphericity(df_results_overall, dv='CorrectAnswer', within=['Meaningful'], subject='ID'))\n",
    "print(pg.sphericity(df_results_overall, dv='CorrectAnswer', within=['TypeAnnotation'], subject='ID'))\n",
    "# print(pg.sphericity(df_results_overall, dv='Time', within=['TypeAnnotation', 'Meaningful'], subject='ID',))\n",
    "# (df_results_overall[['Time', 'TypeAnnotation', 'Meaningful']].corr())#.sum(1).sort_values(ascending=False)\n",
    "# print(stats.bartlett())\n",
    "\n",
    "\n",
    "# piv = df_results_overall.pivot(index='ID', columns=['Meaningful', 'TypeAnnotation'], values='Time')\n",
    "# piv.head()\n",
    "# print(pg.sphericity(piv))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# use statsmodel to analyze the time with variables annotation and meaningful\n",
    "model = smf.mixedlm('Time ~ TypeAnnotation * Meaningful', data=df_results_overall,\n",
    "                    groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for contingency in CorrectAnswer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall['CorrectAnswerNumeric'] = df_results_overall['CorrectAnswer'].apply(lambda x: 1 if x else 0)\n",
    "model = smf.logit('CorrectAnswerNumeric ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall,\n",
    "                  groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())\n",
    "print(model.wald_test_terms(scalar=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = smf.mixedlm('Time ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall,\n",
    "                    groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())\n",
    "print(model.wald_test_terms(scalar=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RQ 2.2 Linearity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "list_measure: list[str] = ['VerticalNext',\n",
    "                           'VerticalLater', 'Regression', 'HorizontalLater', 'LineRegression', 'StoryOrder_Naive_Score',\n",
    "                           'StoryOrder_Dynamic_Score', 'StoryOrder_Dynamic_Repetitions', 'SaccadeLength', 'Linearity'\n",
    "                           ]\n",
    "\n",
    "df_results_overall = df_results_overall[df_results_overall['LineRegression'].notna()].reset_index()\n",
    "df_results_overall = df_results_overall[df_results_overall['SaccadeLength'].notna()].reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for measure in list_measure:\n",
    "    print(f'\\nMeasure: {measure}')\n",
    "    if measure not in []:\n",
    "        model = smf.mixedlm(f'{measure} ~ C(TypeAnnotation) * C(Meaningful)', data=df_results_overall,\n",
    "                            groups=df_results_overall['Task']).fit()\n",
    "        print(model.summary())\n",
    "    else:\n",
    "        model = smf.mixedlm(f'{measure} ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall,\n",
    "                            groups=df_results_overall['Task']).fit()\n",
    "    print(model.wald_test_terms(scalar=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RQ 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the difficulty"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# df_results_overall['DifficultyNumeric'] = df_results_overall['Difficulty'].apply(lambda x: 1 if x == 'Very Difficult' else  0)\n",
    "model = smf.logit('DifficultyNumeric ~ C(Meaningful) * C(TypeAnnotation)', data=df_results_overall,\n",
    "                  groups=df_results_overall['Task']).fit()\n",
    "print(model.summary())\n",
    "print(model.wald_test_terms(scalar=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAComprehension influenced by Meaningful?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "Q3 = df_results_overall.Time.quantile(0.75)\n",
    "Q1 = df_results_overall.Time.quantile(0.25)\n",
    "print(df_results_overall.Time.mean())\n",
    "print(df_results_overall.Time.std())\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "threshold = 1.5\n",
    "\n",
    "outliers = df_results_overall[\n",
    "    (df_results_overall['Time'] < Q1 - threshold * IQR) | (df_results_overall['Time'] > Q3 + threshold * IQR)]\n",
    "print(f'{len(outliers.Time)}, {len(df_results_overall.Time)}')\n",
    "sns.boxplot(df_results_overall, y='Time')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous\n",
    "\n",
    "How did the people find themselves in comparison to others?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df1 = df_personal_information.Classmates.value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Classmates',\n",
    "            # hue='TypeAnnotation', \n",
    "            data=df1,\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            )\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OverallExperience"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df1 = df_personal_information.OverallExperience.value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='OverallExperience',\n",
    "            # hue='TypeAnnotation', \n",
    "            data=df1,\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            )\n",
    "\n",
    "plt.xticks(np.arange(5), ['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Years Programming"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# df1 = df_personal_information.OverallExperience.value_counts(normalize=True)\n",
    "# df1 = df1.mul(100)\n",
    "# df1 = df1.rename('percent').reset_index()\n",
    "\n",
    "# print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='YearsProgramming',\n",
    "              # hue='TypeAnnotation',\n",
    "              data=df_personal_information,\n",
    "              # kind='bar',\n",
    "              )\n",
    "\n",
    "# plt.xticks(np.arange(5), ['Very Inexperienced', 'Inexperienced', 'Average', 'Experienced', 'Very Experienced'])\n",
    "# plt.title('Difficulty grouped by Meaningful')\n",
    "# plt.savefig(f'{figure_path}/rq23difficultyLikertCatPlotMeaningful.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall.Linearity.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Snippets with their mean and SD with correctness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = df_results_overall.groupby(['Task', 'TypeAnnotation']).agg({'Time': ['mean', 'std']})\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Currently unnecessary and unused data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df1 = df_results_overall.groupby(['TypeAnnotation', 'Meaningful'])['CorrectAnswer'].value_counts(normalize=True)\n",
    "df1 = df1.mul(100)\n",
    "df1 = df1.rename('percent').reset_index()\n",
    "print(df1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.catplot(x='Meaningful',\n",
    "            hue='TypeAnnotation',\n",
    "            data=df1,\n",
    "            kind='bar',\n",
    "            y='percent',\n",
    "            )\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table for Type Annotations and Meaningful"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "table_data: dict[str, list[str] | str] = {\n",
    "    'Time': ['mean', 'std'],\n",
    "    'CorrectAnswer': ['sum', 'count'],\n",
    "}\n",
    "\n",
    "df_results_overall.groupby(['Task', 'Meaningful', 'TypeAnnotation']).agg(table_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot for the mean length of each task\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='Task', y='Time', data=df_results_overall)\n",
    "plt.title('Mean Time per Task')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean time per task by Type Annotation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot that shows the mean time for each task\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Mean Time per Task')\n",
    "\n",
    "axs[0].boxplot(\n",
    "    x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == True')['Time'] for task in all_tasks],\n",
    "    labels=all_tasks, )\n",
    "axs[0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0].set_title('Mean Time per Task and True Type Annotation')\n",
    "\n",
    "axs[1].boxplot(\n",
    "    x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == False')['Time'] for task in all_tasks],\n",
    "    labels=all_tasks, )\n",
    "axs[1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1].set_title('Mean Time per Task and False Type Annotation')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean time per task by Identifier Name"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot that shows the mean time for each task\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Mean Time per Task')\n",
    "\n",
    "axs[0].boxplot(x=[df_results_overall.query(f'Task == @task and `Meaningful` == True')['Time'] for task in all_tasks],\n",
    "               labels=all_tasks, )\n",
    "axs[0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0].set_title('Mean Time per Task and Meaningful Identifier Names')\n",
    "\n",
    "axs[1].boxplot(x=[df_results_overall.query(f'Task == @task and `Meaningful` == False')['Time'] for task in all_tasks],\n",
    "               labels=all_tasks, )\n",
    "axs[1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1].set_title('Mean Time per Task and Obfuscated Identifier Names')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean time per task and for the combinations of Type Annoations and Identifier Names"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_tasks: list[str] = sorted(df_results_overall['Task'].unique())\n",
    "\n",
    "# create a plot that shows the mean time for each task\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 10), sharey=True)\n",
    "fig.suptitle('Mean Time per Task')\n",
    "\n",
    "axs[0, 0].boxplot(\n",
    "    x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == False and `Meaningful` == True')['Time'] for\n",
    "       task in all_tasks], labels=all_tasks, notch=False)\n",
    "axs[0, 0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0, 0].set_title('Mean Time per Task without Type Annotations and with Meaningful Identifier Names')\n",
    "\n",
    "axs[0, 1].boxplot(\n",
    "    x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == True and `Meaningful` == True')['Time'] for task\n",
    "       in all_tasks], labels=all_tasks, )\n",
    "axs[0, 1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[0, 1].set_title('Mean Time per Task with Type Annotations and with Meaningful Identifier Names')\n",
    "\n",
    "axs[1, 0].boxplot(\n",
    "    x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == False and `Meaningful` == False')['Time'] for\n",
    "       task in all_tasks], labels=all_tasks, )\n",
    "axs[1, 0].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1, 0].set_title('Mean Time per Task without Type Annotations and with Obfuscated Identifier Names')\n",
    "\n",
    "axs[1, 1].boxplot(\n",
    "    x=[df_results_overall.query(f'Task == @task and `TypeAnnotation` == True and `Meaningful` == False')['Time'] for\n",
    "       task in all_tasks], labels=all_tasks, )\n",
    "axs[1, 1].set_xticklabels(all_tasks, rotation=90)\n",
    "axs[1, 1].set_title('Mean Time per Task with Type Annotations and with Obfuscated Identifier Names')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot to describe how long each task took overall and how this is distributed among the participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot that shows how long each participant took grouped by task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the Type Annotation plots in the top row into one bigger plot with comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(df_results_overall.query(f'`Meaningful` == True'), x='Task', y='Time', hue='TypeAnnotation', split=True,\n",
    "               gap=.1, inner=\"quart\", cut=0, order=all_tasks)\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.title('Mean Time per Task with Meaningful Identifier Names')\n",
    "# TODO: uncomment the following line to save the figure\n",
    "# plt.savefig(f'{figure_path}/timePerTaskrq21_meaningful.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(df_results_overall.query(f'`Meaningful` == False'), x='Task', y='Time', hue='TypeAnnotation', split=True,\n",
    "               gap=.1, inner=\"quart\", cut=0, order=all_tasks)\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.title('Mean Time per Task with Obfuscated Identifier Names')\n",
    "\n",
    "# TODO: uncomment the following line to save the figure\n",
    "# plt.savefig(f'{figure_path}/timePerTaskrq21_obfuscated.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What were the Means and the Standard Deviation for meaningful and type annotation?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_results_overall.groupby(['Meaningful', 'TypeAnnotation', ]).agg({'Time': ['mean', 'std']}))\n",
    "print(df_results_overall.groupby(['Meaningful']).agg({'Time': ['mean', 'std']}))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the correctness:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df_results_overall.groupby(['Meaningful', 'TypeAnnotation', ]).agg({'CorrectAnswer': ['sum', 'count']}))\n",
    "print(df_results_overall.groupby(['Meaningful']).agg({'CorrectAnswer': ['sum', 'count']}))\n",
    "print(df_results_overall.groupby(['TypeAnnotation']).agg({'CorrectAnswer': ['sum', 'count']}))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation heatmap and p-values for the time"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_correlation_meantimes_snippets: DataFrame = DataFrame()\n",
    "\n",
    "df_correlation_meantimes_snippets['M-TA'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == True and Meaningful == True').groupby('Task').agg({'Time': 'mean'})[\n",
    "        'Time']\n",
    "df_correlation_meantimes_snippets['L-N'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == False and Meaningful == False').groupby('Task').agg({'Time': 'mean'})[\n",
    "        'Time']\n",
    "df_correlation_meantimes_snippets['L-TA'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == True and Meaningful == False').groupby('Task').agg({'Time': 'mean'})[\n",
    "        'Time']\n",
    "df_correlation_meantimes_snippets['M-N'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == False and Meaningful == True').groupby('Task').agg({'Time': 'mean'})[\n",
    "        'Time']\n",
    "\n",
    "matrix = df_correlation_meantimes_snippets.corr()\n",
    "print(f'P-Values:\\n{calculate_pvalues(df_correlation_meantimes_snippets)}')\n",
    "\n",
    "sns.heatmap(matrix, cmap=\"Greens\", annot=True)\n",
    "plt.title(f'Correlation Mean-Times')\n",
    "# TODO: uncomment the following line to save the figure\n",
    "# plt.savefig(f'{figure_path}/correlation_meantimesrq21.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation and p-values for the correctness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_correlation_correctness_snippets: DataFrame = DataFrame()\n",
    "\n",
    "df_correlation_correctness_snippets['M-TA'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == True and Meaningful == True').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'sum'})[\n",
    "        'CorrectAnswer'] / \\\n",
    "    df_results_overall.query('TypeAnnotation == True and Meaningful == True').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "df_correlation_correctness_snippets['L-N'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == False and Meaningful == False').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'sum'})['CorrectAnswer'] / \\\n",
    "    df_results_overall.query('TypeAnnotation == False and Meaningful == False').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "df_correlation_correctness_snippets['L-TA'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == True and Meaningful == False').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'sum'})['CorrectAnswer'] / \\\n",
    "    df_results_overall.query('TypeAnnotation == True and Meaningful == False').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "df_correlation_correctness_snippets['M-N'] = \\\n",
    "    df_results_overall.query('TypeAnnotation == False and Meaningful == True').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'sum'})['CorrectAnswer'] / \\\n",
    "    df_results_overall.query('TypeAnnotation == False and Meaningful == True').groupby('Task').agg(\n",
    "        {'CorrectAnswer': 'count'})['CorrectAnswer']\n",
    "\n",
    "matrix = df_correlation_correctness_snippets.corr()\n",
    "print(f'P-Values:\\n{calculate_pvalues(df_correlation_correctness_snippets)}')\n",
    "\n",
    "sns.heatmap(matrix, cmap=\"Greens\", annot=True)\n",
    "plt.title(f'Correlation Correctness')\n",
    "# TODO: uncomment the following line to save the figure\n",
    "# plt.savefig(f'{figure_path}/correlation_correctnessrq21.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the mean time with and without type annotations for each participant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# first plot\n",
    "plot_L_M = axs[0].boxplot(\n",
    "    [df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == True')['Time']\n",
    "     for participant in meaningful_participants], labels=meaningful_participants,\n",
    "    positions=np.arange(len(meaningful_participants)) * 2.0 + 0.35, widths=0.6)\n",
    "plot_TA_M = axs[0].boxplot(\n",
    "    [df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == True')['Time'] for\n",
    "     participant in meaningful_participants], labels=meaningful_participants,\n",
    "    positions=np.arange(len(meaningful_participants)) * 2.0 - 0.35, widths=0.6)\n",
    "\n",
    "# first plot settings\n",
    "axs[0].set_title('Mean Time per Participant with Meaningful Identifier Names')\n",
    "axs[0].set_xticks(np.arange(0, len(meaningful_participants) * 2, 2), meaningful_participants, rotation=90)\n",
    "define_box_properties(plot_TA_M, 'blue', 'TypeAnnotation', axs[0])\n",
    "define_box_properties(plot_L_M, 'orange', 'No Type Annotation', axs[0])\n",
    "\n",
    "# second plot\n",
    "plot_L_L = axs[1].boxplot(\n",
    "    [df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == False')['Time']\n",
    "     for participant in obfuscated_participants], labels=obfuscated_participants,\n",
    "    positions=np.arange(len(obfuscated_participants)) * 2.0 + 0.35, widths=0.6)\n",
    "plot_TA_L = axs[1].boxplot(\n",
    "    [df_results_overall.query(f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == False')['Time']\n",
    "     for participant in obfuscated_participants], labels=obfuscated_participants,\n",
    "    positions=np.arange(len(obfuscated_participants)) * 2.0 - 0.35, widths=0.6)\n",
    "\n",
    "# second plot settings\n",
    "axs[1].set_title('Mean Time per Participant with Obfuscated Identifier Names')\n",
    "axs[1].set_xticks(np.arange(0, len(obfuscated_participants) * 2, 2), obfuscated_participants, rotation=90)\n",
    "define_box_properties(plot_TA_L, 'blue', 'TypeAnnotation', axs[1])\n",
    "define_box_properties(plot_L_L, 'orange', 'No Type Annotation', axs[1])\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long people took overall."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall.groupby(['Meaningful', ]).agg({'Time': 'sum'}).unstack().T.plot(kind='bar', stacked=True,\n",
    "                                                                                   figsize=(15, 10))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `meaningful` snippets make participants faster on average?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compute if meaningful snippets were faster on average than obfuscated snippets\n",
    "df_results_overall.groupby(['Meaningful']).agg({'Time': 'mean'}).unstack().T.plot(kind='bar', stacked=False,\n",
    "                                                                                  figsize=(15, 10))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `Type Annotations` make people faster on average?\n",
    "\n",
    "- [ ] TODO: Apparently, this is not the case. Why could this be? More to read? Where do people look during this time? What are they focusing on? Are they more correct?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# compute if type annotated snippets were faster on average than non type annotated snippets\n",
    "df_results_overall.groupby(['TypeAnnotation']).agg({'Time': 'mean'}).unstack().T.plot(kind='bar', stacked=True,\n",
    "                                                                                      figsize=(15, 10))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `Type Annotations` make people faster when we differentiate between `Meaningful` snippets?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall.groupby(['Meaningful', 'TypeAnnotation']).agg({'Time': 'mean'}).unstack().T.plot(kind='bar',\n",
    "                                                                                                    figsize=(15, 10))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are snippets with `Type Annotations` more correct?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "snippets_grouping: list[str | list[str]] = ['TypeAnnotation', 'Meaningful', ['TypeAnnotation', 'Meaningful'], ]\n",
    "\n",
    "# create a plot consisting of len(snippets_grouping) subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, x in enumerate(snippets_grouping):\n",
    "    ax = df_results_overall.groupby(x).agg({'CorrectAnswer': np.count_nonzero}).unstack().T.plot(kind='bar',\n",
    "                                                                                                 ax=axs[i // 2, i % 2],\n",
    "                                                                                                 title=(lambda\n",
    "                                                                                                            x: f'{x[0]} and {x[1]}' if len(\n",
    "                                                                                                     x) == 2 else x)(x),\n",
    "                                                                                                 ylabel='Number of Correct Answers',\n",
    "                                                                                                 legend=False)\n",
    "\n",
    "    ax.set_xticklabels(['False', 'True'], rotation=0)\n",
    "\n",
    "    if len(x) != 2:\n",
    "        ax.axhline(y=df_results_overall.groupby(x).agg({'CorrectAnswer': 'count'}).unstack()[0], color='orange',\n",
    "                   linestyle='--', label='Max False')  # False\n",
    "        ax.axhline(y=df_results_overall.groupby(x).agg({'CorrectAnswer': 'count'}).unstack()[1], color='green',\n",
    "                   linestyle='--', label='Max True')  # True\n",
    "        ax.legend(loc='lower center')\n",
    "\n",
    "    for line in ax.lines:\n",
    "        ax.annotate(str(int(line.get_ydata()[0])), xy=(line.get_xdata()[0], line.get_ydata()[0]), xytext=(280, 2),\n",
    "                    textcoords='offset points', ha='right', va='bottom', fontsize=8)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center',\n",
    "                    xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are snippets that are `Meaningful` more correct?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_results_overall.groupby(['TypeAnnotation', 'Meaningful']).agg({'CorrectAnswer': 'count'}).unstack()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better representation of this:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "correct_TA_M: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == True')['CorrectAnswer'].sum()\n",
    "correct_TA_L: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == False')['CorrectAnswer'].sum()\n",
    "correct_L_M: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == True')['CorrectAnswer'].sum()\n",
    "correct_L_L: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == False')['CorrectAnswer'].sum()\n",
    "\n",
    "total_TA_M: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == True')['CorrectAnswer'].count()\n",
    "total_TA_L: int = df_results_overall.query('`TypeAnnotation` == True and Meaningful == False')['CorrectAnswer'].count()\n",
    "total_L_M: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == True')['CorrectAnswer'].count()\n",
    "total_L_L: int = df_results_overall.query('`TypeAnnotation` == False and Meaningful == False')['CorrectAnswer'].count()\n",
    "\n",
    "print(f'Type Annotation and Meaningful: {correct_TA_M}/{total_TA_M} = {correct_TA_M / total_TA_M}')\n",
    "print(f'Type Annotation and Obfuscated: {correct_TA_L}/{total_TA_L} = {correct_TA_L / total_TA_L}')\n",
    "print(f'No Type Annotation and Meaningful: {correct_L_M}/{total_L_M} = {correct_L_M / total_L_M}')\n",
    "print(f'No Type Annotation and Obfuscated: {correct_L_L}/{total_L_L} = {correct_L_L / total_L_L}')\n",
    "\n",
    "print(\n",
    "    f'\\nType Annotation: {correct_TA_M + correct_TA_L}/{total_TA_M + total_TA_L} = {(correct_TA_M + correct_TA_L) / (total_TA_M + total_TA_L)}')\n",
    "print(\n",
    "    f'No Type Annotation: {correct_L_M + correct_L_L}/{total_L_M + total_L_L} = {(correct_L_M + correct_L_L) / (total_L_M + total_L_L)}')\n",
    "print(\n",
    "    f'Meaningful: {correct_TA_M + correct_L_M}/{total_TA_M + total_L_M} = {(correct_TA_M + correct_L_M) / (total_TA_M + total_L_M)}')\n",
    "print(\n",
    "    f'Obfuscated: {correct_TA_L + correct_L_L}/{total_TA_L + total_L_L} = {(correct_TA_L + correct_L_L) / (total_TA_L + total_L_L)}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Answers per Snippet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "snippet_correctness_df: DataFrame = DataFrame(columns=['Task', 'Correctness', 'Meaningful', 'TypeAnnotation'])\n",
    "\n",
    "counter: int = 0\n",
    "\n",
    "for task in all_tasks:\n",
    "    correct_TA_M: int = \\\n",
    "        df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == True')[\n",
    "            'CorrectAnswer'].sum()\n",
    "    correct_TA_L: int = \\\n",
    "        df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == False')[\n",
    "            'CorrectAnswer'].sum()\n",
    "    correct_L_M: int = \\\n",
    "        df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == True')[\n",
    "            'CorrectAnswer'].sum()\n",
    "    correct_L_L: int = \\\n",
    "        df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == False')[\n",
    "            'CorrectAnswer'].sum()\n",
    "\n",
    "    total_TA_M: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == True')[\n",
    "        'CorrectAnswer'].count()\n",
    "    total_TA_L: int = \\\n",
    "        df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == True and Meaningful == False')[\n",
    "            'CorrectAnswer'].count()\n",
    "    total_L_M: int = df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == True')[\n",
    "        'CorrectAnswer'].count()\n",
    "    total_L_L: int = \\\n",
    "        df_results_overall.query(f'Task == \"{task}\" and `TypeAnnotation` == False and Meaningful == False')[\n",
    "            'CorrectAnswer'].count()\n",
    "\n",
    "    print(f'\\nTask: {task}')\n",
    "\n",
    "    print(\n",
    "        f'\\nNo Type Annotation: {correct_L_M + correct_L_L}/{total_L_M + total_L_L} = {(correct_L_M + correct_L_L) / (total_L_M + total_L_L)}')\n",
    "    print(\n",
    "        f'Type Annotation: {correct_TA_M + correct_TA_L}/{total_TA_M + total_TA_L} = {(correct_TA_M + correct_TA_L) / (total_TA_M + total_TA_L)}')\n",
    "\n",
    "    snippet_correctness_df.loc[counter] = [task, correct_TA_M / total_TA_M, True, True]\n",
    "    snippet_correctness_df.loc[counter + 1] = [task, correct_TA_L / total_TA_L, False, True]\n",
    "    snippet_correctness_df.loc[counter + 2] = [task, correct_L_M / total_L_M, True, False]\n",
    "    snippet_correctness_df.loc[counter + 3] = [task, correct_L_L / total_L_L, False, False]\n",
    "\n",
    "    counter += 4"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistic for the participants:\n",
    "- Number of Snippets\n",
    "- Number of Correct / Incorrect Snippets\n",
    "- Meaningful / Obfuscated Snippets\n",
    "- Mean Time for Type Annotation / No Type Annotation\n",
    "- Overall Time Taken in Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the participants in the obfuscated group."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f'Obfuscated Participants')\n",
    "for participant in obfuscated_participants:\n",
    "    print(f'\\nParticipant {participant}')\n",
    "    number_of_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].count()\n",
    "    correct_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].sum()\n",
    "    print(\n",
    "        f'Participant {participant} has {correct_snippets}/{number_of_snippets} correct snippets: {correct_snippets / number_of_snippets}')\n",
    "\n",
    "    mean_time: float = df_results_overall.query(f'ID == \"{participant}\"')['Time'].mean()\n",
    "    print(f'Participant {participant} has a mean time of {mean_time} seconds')\n",
    "\n",
    "    mean_time_TA: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['Time'].mean()\n",
    "    mean_time_L: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['Time'].mean()\n",
    "    print(\n",
    "        f'Participant {participant} has a mean time of {mean_time_TA} seconds with Type Annotations and {mean_time_L} seconds without Type Annotations, thus the change is {mean_time_L - mean_time_TA} seconds')\n",
    "\n",
    "    overall_time: float = df_results_overall.query(f'ID == @participant')['Time'].sum()\n",
    "    print(f'Participant {participant} took {overall_time // 60} minutes and {overall_time % 60} seconds in total')\n",
    "\n",
    "print(f'\\n\\nMeaningful Participants')\n",
    "\n",
    "for participant in meaningful_participants:\n",
    "    print(f'\\nParticipant {participant}')\n",
    "    number_of_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].count()\n",
    "    correct_snippets: int = df_results_overall.query(f'ID == \"{participant}\"')['CorrectAnswer'].sum()\n",
    "    print(\n",
    "        f'Participant {participant} has {correct_snippets}/{number_of_snippets} correct snippets: {correct_snippets / number_of_snippets}')\n",
    "\n",
    "    mean_time: float = df_results_overall.query(f'ID == \"{participant}\"')['Time'].mean()\n",
    "    print(f'Participant {participant} has a mean time of {mean_time} seconds')\n",
    "\n",
    "    mean_time_TA: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')['Time'].mean()\n",
    "    mean_time_L: float = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')['Time'].mean()\n",
    "    print(\n",
    "        f'Participant {participant} has a mean time of {mean_time_TA} seconds with Type Annotations and {mean_time_L} seconds without Type Annotations, thus the change is {mean_time_L - mean_time_TA} seconds')\n",
    "\n",
    "    overall_time: float = df_results_overall.query(f'ID == @participant')['Time'].sum()\n",
    "    print(f'Participant {participant} took {overall_time // 60} minutes and {overall_time % 60} seconds in total')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the number of correct snippets as a ratio for participants."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "correctness_df_by_participant_meaningful: DataFrame = DataFrame(columns=['ID', 'Correctness', 'TypeAnnotation'])\n",
    "correctness_df_by_participant_obfuscated: DataFrame = DataFrame(columns=['ID', 'Correctness', 'TypeAnnotation'])\n",
    "\n",
    "counter: int = 0\n",
    "for participant in meaningful_participants:\n",
    "    correct_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')[\n",
    "        'CorrectAnswer'].sum()\n",
    "    correct_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')[\n",
    "        'CorrectAnswer'].sum()\n",
    "\n",
    "    total_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')[\n",
    "        'CorrectAnswer'].count()\n",
    "    total_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')[\n",
    "        'CorrectAnswer'].count()\n",
    "\n",
    "    correctness_df_by_participant_meaningful.loc[counter] = [participant, correct_TA / total_TA, True]\n",
    "    correctness_df_by_participant_meaningful.loc[counter + 1] = [participant, correct_L / total_L, False]\n",
    "\n",
    "    counter += 2\n",
    "\n",
    "counter = 0\n",
    "for participant in obfuscated_participants:\n",
    "    correct_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')[\n",
    "        'CorrectAnswer'].sum()\n",
    "    correct_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')[\n",
    "        'CorrectAnswer'].sum()\n",
    "\n",
    "    total_TA: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == True')[\n",
    "        'CorrectAnswer'].count()\n",
    "    total_L: int = df_results_overall.query(f'ID == \"{participant}\" and `TypeAnnotation` == False')[\n",
    "        'CorrectAnswer'].count()\n",
    "\n",
    "    correctness_df_by_participant_obfuscated.loc[counter] = [participant, correct_TA / total_TA, True]\n",
    "    correctness_df_by_participant_obfuscated.loc[counter + 1] = [participant, correct_L / total_L, False]\n",
    "\n",
    "    counter += 2\n",
    "\n",
    "correctness_df_by_participant_meaningful.groupby(['TypeAnnotation', 'ID']).agg(\n",
    "    {'Correctness': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10), title=\"Meaningful\")\n",
    "\n",
    "correctness_df_by_participant_obfuscated.groupby(['TypeAnnotation', 'ID']).agg(\n",
    "    {'Correctness': 'mean'}).unstack().T.plot(kind='bar', figsize=(15, 10), title=\"Obfuscated\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the time and correct answers per code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "snippet_correctness_df.groupby(['Meaningful', 'TypeAnnotation', 'Task']).agg({'Correctness': 'mean'}).unstack().T.plot(\n",
    "    kind='bar', figsize=(15, 10))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty Rating\n",
    "\n",
    "First let's check how the difficulty is for each task. I think this would be best as a table?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(df_results_overall.query(f'`Meaningful` == True'), x='Task', y='Difficulty', hue='TypeAnnotation',\n",
    "            gap=.1,\n",
    "            # inner=\"quart\", \n",
    "            # cut=0, \n",
    "            order=all_tasks,\n",
    "            # split=True,\n",
    "            )\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "plt.title('Difficulty per Task with Meaningful Identifier Names')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now with obfuscated identifier names."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.violinplot(df_results_overall.query(f'`Meaningful` == False'), x='Task', y='Difficulty',\n",
    "               hue='TypeAnnotation',\n",
    "               gap=.1,\n",
    "               inner=\"quart\",\n",
    "               cut=0,\n",
    "               order=all_tasks,\n",
    "               split=True,\n",
    "               )\n",
    "\n",
    "plt.xticks(all_tasks, rotation=90)\n",
    "plt.yticks(np.arange(1, 6, 1))\n",
    "plt.title('Difficulty per Task with and with Obfuscated Identifier Names')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty that each participant felt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(20, 10), sharey=True)\n",
    "\n",
    "# first plot\n",
    "plot_L_M = axs[0].boxplot([df_results_overall.query(\n",
    "    f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == True')['Difficulty'] for participant in\n",
    "                           meaningful_participants], labels=meaningful_participants,\n",
    "                          positions=np.arange(len(meaningful_participants)) * 2.0 + 0.35, widths=0.6)\n",
    "plot_TA_M = axs[0].boxplot([df_results_overall.query(\n",
    "    f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == True')['Difficulty'] for participant in\n",
    "                            meaningful_participants], labels=meaningful_participants,\n",
    "                           positions=np.arange(len(meaningful_participants)) * 2.0 - 0.35, widths=0.6)\n",
    "\n",
    "# first plot settings\n",
    "axs[0].set_title('Mean Difficulty per Participant with Meaningful Identifier Names')\n",
    "axs[0].set_xticks(np.arange(0, len(meaningful_participants) * 2, 2), meaningful_participants, rotation=90)\n",
    "axs[0].set_yticks(np.arange(1, 6, 1))\n",
    "define_box_properties(plot_TA_M, 'blue', 'Type Annotation', axs[0])\n",
    "define_box_properties(plot_L_M, 'orange', 'No Type Annotation', axs[0])\n",
    "\n",
    "# second plot\n",
    "plot_L_L = axs[1].boxplot([df_results_overall.query(\n",
    "    f'`ID` == @participant and `TypeAnnotation` == False and `Meaningful` == False')['Difficulty'] for participant in\n",
    "                           obfuscated_participants], labels=obfuscated_participants,\n",
    "                          positions=np.arange(len(obfuscated_participants)) * 2.0 + 0.35, widths=0.6)\n",
    "plot_TA_L = axs[1].boxplot([df_results_overall.query(\n",
    "    f'`ID` == @participant and `TypeAnnotation` == True and `Meaningful` == False')['Difficulty'] for participant in\n",
    "                            obfuscated_participants], labels=obfuscated_participants,\n",
    "                           positions=np.arange(len(obfuscated_participants)) * 2.0 - 0.35, widths=0.6)\n",
    "\n",
    "# second plot settings\n",
    "axs[1].set_title('Mean Difficulty per Participant with Obfuscated Identifier Names')\n",
    "axs[1].set_xticks(np.arange(0, len(obfuscated_participants) * 2, 2), obfuscated_participants, rotation=90)\n",
    "axs[1].set_yticks(np.arange(1, 6, 1))\n",
    "define_box_properties(plot_TA_L, 'blue', 'Type Annotation', axs[1])\n",
    "define_box_properties(plot_L_L, 'orange', 'No Type Annotation', axs[1])\n",
    "\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct Answer by Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "snippet_correctness_df.groupby(['Meaningful', 'TypeAnnotation']).agg({'Correctness': 'mean'}).unstack().T.plot(\n",
    "    kind='bar', figsize=(15, 10))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulty count by snippet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "\n",
    "g = sns.FacetGrid(df_results_overall, row=\"Task\",\n",
    "                  #   col=\"Type Annotation\",\n",
    "                  hue=\"Task\",\n",
    "                  height=1,\n",
    "                  aspect=30,\n",
    "                  palette=pal,\n",
    "                  )\n",
    "g.map_dataframe(sns.histplot, x=\"Difficulty\", hue=\"TypeAnnotation\", binwidth=1, binrange=(1, 6), multiple=\"stack\")\n",
    "# g.map_dataframe(sns.kdeplot, x=\"Difficulty\", clip=(1, 5), fill=True)\n",
    "# g.map_dataframe(sns.kdeplot, x=\"Difficulty\", hue='Type Annotation', clip_on=(1, 5), color=\"black\", lw=2, bw_adjust=.5)\n",
    "\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "g.map(label, \"Task\")\n",
    "g.figure.subplots_adjust(hspace=-.5)\n",
    "\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.set(xticks=np.arange(1, 6, 1))\n",
    "g.despine(bottom=True, left=True)\n",
    "\n",
    "g.figure.tight_layout()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display META Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# if the line 'Total' does not exist, create it\n",
    "if not 'Total' in df_meta_data.index:\n",
    "    df_meta_data.loc['Total'] = df_meta_data[\n",
    "        [col for col in meta_data_columns if col not in unnecessary_columns_meta_data]].sum()\n",
    "    # add the number of times a participant did not finish all snippets to the 'Total' line\n",
    "    df_meta_data.loc['Total', 'NumberOfMissingSnippets'] = \\\n",
    "        df_meta_data.groupby('ID')['NumberOfMissingSnippets'].any().value_counts()[True]\n",
    "    # df_meta_data.loc['Total']['NumberOfMissingSnippets'] = df_meta_data['NumberOfMissingSnippets'].any()\n",
    "    df_meta_data = df_meta_data.fillna('')\n",
    "\n",
    "# set the background color of the dataframe value to red if the value is False\n",
    "df_meta_data.style.applymap(lambda x: 'background-color: darkred' if x == False else '')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The corrected p-values for the mixed linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Example p-values\n",
    "p_values = np.array([0.43, 0.033, 0.056, 0.031, 0.33, 0.1, 0.55, 0.97])\n",
    "\n",
    "# Perform Benjamini-Hochberg FDR correction\n",
    "alpha = 0.05\n",
    "rejected, pvals_corrected, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "\n",
    "# Output the results\n",
    "print(\"Original p-values:\", p_values)\n",
    "print(\"Corrected p-values:\", pvals_corrected)\n",
    "print(\"Rejected hypotheses:\", rejected)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Example p-values\n",
    "p_values = np.array([0.43, 0.033, 0.056, 0.031, 0.33, 0.1, 0.55, 0.97])\n",
    "\n",
    "# Perform Benjamini-Hochberg FDR correction\n",
    "alpha = 0.05\n",
    "rejected, pvals_corrected, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
    "\n",
    "# Output the results\n",
    "print(\"Original p-values:\", p_values)\n",
    "print(\"Corrected p-values:\", pvals_corrected)\n",
    "print(\"Rejected hypotheses:\", rejected)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
